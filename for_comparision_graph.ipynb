{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2191a9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "with open('/Users/pranathi/Library/CloudStorage/OneDrive-ErasmusUniversityRotterdam/Masters/computer_science/TVs-all-merged.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "\n",
    "\n",
    "all_values = [item for sublist in data.values() for item in sublist]    \n",
    "all_values_dict = {i: item for i, item in enumerate(all_values)}\n",
    "\n",
    "def clean_all_values_dict_advanced(all_values_dict, remove_shops=True):\n",
    "   \n",
    "    def clean_text(text, remove_shops=True):\n",
    "        if not isinstance(text, str):\n",
    "            return text\n",
    "        \n",
    "        cleaned = text.lower()  # Lowercase first\n",
    "        \n",
    "        \n",
    "        cleaned = re.sub(r'\\s+', '', cleaned)  # Remove all spaces early\n",
    "        \n",
    "        \n",
    "        inch_patterns = [\n",
    "            (r'[\\'\"]{1,2}', 'inch'),           # \" or ' or '' → inch\n",
    "            (r'-inch', 'inch'),                 # -inch → inch (no \\b needed after space removal)\n",
    "            (r'inches', 'inch'),                # inches → inch\n",
    "            (r'(\\d+)in(?=\\d|$)', r'\\1inch'),    # 40in → 40inch (standalone \"in\")\n",
    "        ]\n",
    "        for pattern, replacement in inch_patterns:\n",
    "            cleaned = re.sub(pattern, replacement, cleaned)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # STEP 3: Normalize HERTZ variations (simplified)\n",
    "        # =====================================================================\n",
    "        hz_patterns = [\n",
    "            (r'hertz', 'hz'),                   # hertz → hz\n",
    "            (r'-hz', 'hz'),                     # -hz → hz\n",
    "        ]\n",
    "        for pattern, replacement in hz_patterns:\n",
    "            cleaned = re.sub(pattern, replacement, cleaned)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # STEP 4: Normalize RESOLUTION patterns (simplified)\n",
    "        # =====================================================================\n",
    "        resolution_patterns = [\n",
    "            (r'4k', '2160p'),                   # 4k → 2160p\n",
    "            (r'uhd', '2160p'),                  # uhd → 2160p\n",
    "            (r'fullhd', '1080p'),               # fullhd → 1080p (spaces already removed)\n",
    "            (r'hd(?=tv)', '720p'),              # hd before tv → 720p (but keep hdtv)\n",
    "        ]\n",
    "        for pattern, replacement in resolution_patterns:\n",
    "            cleaned = re.sub(pattern, replacement, cleaned)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # STEP 5: Normalize LED/LCD/PLASMA/OLED (simplified)\n",
    "        # =====================================================================\n",
    "        display_patterns = [\n",
    "            (r'led-lcd', 'led'),                # led-lcd → led\n",
    "            (r'lcd/led', 'led'),                # lcd/led → led\n",
    "            (r'led/lcd', 'led'),                # led/lcd → led\n",
    "        ]\n",
    "        for pattern, replacement in display_patterns:\n",
    "            cleaned = re.sub(pattern, replacement, cleaned)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # STEP 6: Remove shop/website-specific noise (OPTIONAL)\n",
    "        # =====================================================================\n",
    "        # Only remove shops if remove_shops=True\n",
    "        if remove_shops:\n",
    "            website_noise = [\n",
    "                # Main 4 shops - with domain extensions (most specific)\n",
    "                r'newegg\\.?com',\n",
    "                r'amazon\\.?com',\n",
    "                r'bestbuy\\.?com',\n",
    "                r'best-buy\\.?com',\n",
    "                r'thenerds\\.?net',\n",
    "                # Other common shops\n",
    "                \n",
    "                # Shop names as standalone words (word boundaries)\n",
    "                r'\\bnewegg\\b',\n",
    "                r'\\bamazon\\b',\n",
    "                r'\\bbestbuy\\b',\n",
    "                r'\\bbest\\s*buy\\b',\n",
    "                r'\\bthenerds\\b',\n",
    "                # Shop names at start/end (even without word boundaries after space removal)\n",
    "                r'^newegg',\n",
    "                r'^amazon',\n",
    "                r'^bestbuy',\n",
    "                r'^bestbuy',\n",
    "                r'^thenerds',\n",
    "                r'newegg$',\n",
    "                r'amazon$',\n",
    "                r'bestbuy$',  # Catches \"bestbuy\" at end (common pattern)\n",
    "                r'bestbuy$',\n",
    "                r'thenerds$',\n",
    "                # Common prefixes/suffixes\n",
    "                r'^refurbished:?',\n",
    "                r'^certifiedrefurbished:?',\n",
    "                r'-best\\s*buy',\n",
    "                r'-newegg',\n",
    "                r'-amazon',\n",
    "                r'best\\s*buy$',\n",
    "            ]\n",
    "            for pattern in website_noise:\n",
    "                cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)\n",
    "            \n",
    "            # Additional cleanup: Remove any remaining shop fragments\n",
    "            # This catches cases where shop name got partially removed or concatenated\n",
    "            shop_fragments = ['newegg', 'amazon', 'bestbuy', 'thenerds']\n",
    "            for fragment in shop_fragments:\n",
    "                # Remove if it appears as a standalone word or at boundaries\n",
    "                # This is especially important after space removal when shop names get concatenated\n",
    "                cleaned = re.sub(rf'\\b{fragment}\\b', '', cleaned, flags=re.IGNORECASE)\n",
    "                cleaned = re.sub(rf'^{fragment}', '', cleaned, flags=re.IGNORECASE)\n",
    "                cleaned = re.sub(rf'{fragment}$', '', cleaned, flags=re.IGNORECASE)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # STEP 7: Remove noise characters (punctuation, special chars)\n",
    "        # =====================================================================\n",
    "        cleaned = re.sub(r'[^\\w]', '', cleaned)  # Remove all non-alphanumeric\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    def clean_dict(d, remove_shops=True):\n",
    "        cleaned = {}\n",
    "        for k, v in d.items():\n",
    "            # SKIP modelID completely - preserve original key and value\n",
    "            if k.lower() == 'modelid':\n",
    "                cleaned[k] = v\n",
    "                continue\n",
    "            \n",
    "            key_clean = clean_text(k, remove_shops=remove_shops)\n",
    "            if isinstance(v, str):\n",
    "                cleaned[key_clean] = clean_text(v, remove_shops=remove_shops)\n",
    "            elif isinstance(v, dict):\n",
    "                cleaned[key_clean] = clean_dict(v, remove_shops=remove_shops)\n",
    "            else:\n",
    "                cleaned[key_clean] = v\n",
    "        return cleaned\n",
    "    \n",
    "    return {idx: clean_dict(item, remove_shops=remove_shops) for idx, item in all_values_dict.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "count_duplicate_pairs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DUPLICATE PAIRS ANALYSIS (Based on modelID)\n",
      "======================================================================\n",
      "\n",
      "Total products in dataset: 1624\n",
      "Unique modelIDs: 1262\n",
      "ModelIDs with duplicates (2+ products): 329\n",
      "ModelIDs with single product (no duplicates): 933\n",
      "\n",
      "Total duplicate pairs: 399\n",
      "\n",
      "Cluster size distribution:\n",
      "  Mean cluster size: 2.10\n",
      "  Min cluster size: 2\n",
      "  Max cluster size: 4\n",
      "\n",
      "Cluster size frequency:\n",
      "  Size 2: 300 clusters (300 pairs)\n",
      "  Size 3: 25 clusters (75 pairs)\n",
      "  Size 4: 4 clusters (24 pairs)\n",
      "\n",
      "Fraction of all possible pairs that are duplicates:\n",
      "  0.000303 (0.0303%)\n",
      "  (Total possible pairs: 1,317,876)\n",
      "  (Duplicate pairs: 399)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COUNT TOTAL DUPLICATE PAIRS IN ORIGINAL DATA (Using modelID)\n",
    "# ============================================================================\n",
    "from collections import defaultdict\n",
    "\n",
    "# Build ground truth clusters from modelID\n",
    "modelID_to_indices = defaultdict(list)\n",
    "\n",
    "for idx, item in all_values_dict.items():\n",
    "    if 'modelID' in item and item['modelID']:\n",
    "        modelID_to_indices[item['modelID']].append(idx)\n",
    "\n",
    "# Keep only modelIDs with 2+ products (actual duplicates)\n",
    "duplicate_clusters = {mid: set(idxs) for mid, idxs in modelID_to_indices.items() if len(idxs) > 1}\n",
    "\n",
    "# Calculate total duplicate pairs\n",
    "# For each cluster with n products, there are n*(n-1)/2 pairs\n",
    "total_duplicate_pairs = 0\n",
    "cluster_sizes = []\n",
    "\n",
    "for modelID, indices in duplicate_clusters.items():\n",
    "    n = len(indices)\n",
    "    pairs_in_cluster = n * (n - 1) // 2\n",
    "    total_duplicate_pairs += pairs_in_cluster\n",
    "    cluster_sizes.append(n)\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*70)\n",
    "print(\"DUPLICATE PAIRS ANALYSIS (Based on modelID)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTotal products in dataset: {len(all_values_dict)}\")\n",
    "print(f\"Unique modelIDs: {len(modelID_to_indices)}\")\n",
    "print(f\"ModelIDs with duplicates (2+ products): {len(duplicate_clusters)}\")\n",
    "print(f\"ModelIDs with single product (no duplicates): {len(modelID_to_indices) - len(duplicate_clusters)}\")\n",
    "print(f\"\\nTotal duplicate pairs: {total_duplicate_pairs}\")\n",
    "print(f\"\\nCluster size distribution:\")\n",
    "print(f\"  Mean cluster size: {sum(cluster_sizes) / len(cluster_sizes) if cluster_sizes else 0:.2f}\")\n",
    "print(f\"  Min cluster size: {min(cluster_sizes) if cluster_sizes else 0}\")\n",
    "print(f\"  Max cluster size: {max(cluster_sizes) if cluster_sizes else 0}\")\n",
    "\n",
    "# Show distribution of cluster sizes\n",
    "from collections import Counter\n",
    "size_distribution = Counter(cluster_sizes)\n",
    "print(f\"\\nCluster size frequency:\")\n",
    "for size in sorted(size_distribution.keys()):\n",
    "    count = size_distribution[size]\n",
    "    pairs = size * (size - 1) // 2\n",
    "    print(f\"  Size {size}: {count} clusters ({pairs * count} pairs)\")\n",
    "\n",
    "# Calculate what fraction of all possible pairs are duplicates\n",
    "total_possible_pairs = len(all_values_dict) * (len(all_values_dict) - 1) // 2\n",
    "fraction_duplicates = total_duplicate_pairs / total_possible_pairs if total_possible_pairs > 0 else 0\n",
    "\n",
    "print(f\"\\nFraction of all possible pairs that are duplicates:\")\n",
    "print(f\"  {fraction_duplicates:.6f} ({fraction_duplicates*100:.4f}%)\")\n",
    "print(f\"  (Total possible pairs: {total_possible_pairs:,})\")\n",
    "print(f\"  (Duplicate pairs: {total_duplicate_pairs:,})\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Store for later use\n",
    "ground_truth_duplicate_pairs = total_duplicate_pairs\n",
    "ground_truth_duplicate_clusters = duplicate_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce9a465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def bootstrap_train_test_split(n_samples):\n",
    "    \n",
    "    # Draw bootstrap sample with replacement using sklearn\n",
    "    train_indices = resample(range(n_samples), n_samples=n_samples, \n",
    "                            replace=True)\n",
    "    train_indices = np.array(train_indices)\n",
    "    \n",
    "    # Out-of-sample instances (not selected in bootstrap)\n",
    "    all_indices = set(range(n_samples))\n",
    "    train_set = set(train_indices)\n",
    "    test_indices = np.array(sorted(all_indices - train_set))\n",
    "    \n",
    "    return train_indices, test_indices\n",
    "\n",
    "def run_bootstrap_iterations(n_samples, n_bootstraps=5):\n",
    "    \n",
    "    bootstrap_splits = []\n",
    "    \n",
    "    for i in range(n_bootstraps):\n",
    "        train_idx, test_idx = bootstrap_train_test_split(n_samples)\n",
    "        bootstrap_splits.append((train_idx, test_idx))\n",
    "        \n",
    "        if (i + 1) % 10 == 0 or i == 0:\n",
    "            train_pct = len(np.unique(train_idx)) / n_samples * 100\n",
    "            test_pct = len(test_idx) / n_samples * 100\n",
    "            print(f\"Bootstrap {i+1}/{n_bootstraps}: \"\n",
    "                  f\"{len(np.unique(train_idx))} unique train ({train_pct:.1f}%), \"\n",
    "                  f\"{len(test_idx)} test ({test_pct:.1f}%)\")\n",
    "    \n",
    "    return bootstrap_splits\n",
    "\n",
    "def apply_bootstrap_to_data(data, train_indices, test_indices):\n",
    "    \n",
    "    if isinstance(data, np.ndarray):\n",
    "        train_data = data[train_indices]\n",
    "        test_data = data[test_indices]\n",
    "    elif isinstance(data, list):\n",
    "        train_data = [data[i] for i in train_indices]\n",
    "        test_data = [data[i] for i in test_indices]\n",
    "    else:  # Assume pandas DataFrame/Series\n",
    "        train_data = data.iloc[train_indices]\n",
    "        test_data = data.iloc[test_indices]\n",
    "    \n",
    "    return train_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a1d3b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def split_and_extract_model_words(title):\n",
    "    title = title.lower()\n",
    "\n",
    "    # 1) Find base model tokens containing both letters and digits\n",
    "    base_pattern = r\"[a-z0-9]*(([0-9]+[^0-9,\\s]+)|([^0-9,\\s]+[0-9]+))[a-z0-9]*\"\n",
    "    base_tokens = [m.group(0) for m in re.finditer(base_pattern, title)]\n",
    "\n",
    "    refined_tokens = []\n",
    "    for tok in base_tokens:\n",
    "        # 2) Split token further into pieces at digit/letter boundaries\n",
    "        #    e.g. \"32inch720p\" -> [\"32inch\", \"720p\"]\n",
    "        parts = re.findall(r\"[0-9]+[a-z]+|[a-z]+[0-9]+|[0-9]+|[a-z]+\", tok)\n",
    "        # keep only parts that contain both letters and digits (model-like)\n",
    "        for p in parts:\n",
    "            if re.search(r\"[a-z]\", p) and re.search(r\"[0-9]\", p):\n",
    "                refined_tokens.append(p)\n",
    "\n",
    "    return {\"model_words\": refined_tokens}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_model_words_from_title(title):\n",
    "   \n",
    "    if not isinstance(title, str):\n",
    "        return set()\n",
    "    \n",
    "    title = title.lower()\n",
    "    \n",
    "    # Paper's pattern for title model words\n",
    "    base_pattern = r\"[a-z0-9]*(([0-9]+[^0-9,\\s]+)|([^0-9,\\s]+[0-9]+))[a-z0-9]*\"\n",
    "    base_tokens = [m.group(0) for m in re.finditer(base_pattern, title)]\n",
    "    \n",
    "    refined_tokens = []\n",
    "    for tok in base_tokens:\n",
    "        # Split token further at digit/letter boundaries\n",
    "        parts = re.findall(r\"[0-9]+[a-z]+|[a-z]+[0-9]+|[0-9]+|[a-z]+\", tok)\n",
    "        for p in parts:\n",
    "            if re.search(r\"[a-z]\", p) and re.search(r\"[0-9]\", p):\n",
    "                refined_tokens.append(p)\n",
    "    \n",
    "    return set(refined_tokens)\n",
    "\n",
    "\n",
    "def extract_model_words_from_value(value):\n",
    "    \n",
    "    if not isinstance(value, str):\n",
    "        return set()\n",
    "    \n",
    "    value = value.lower().strip()\n",
    "    model_words = set()\n",
    "    \n",
    "    # Split by common separators to handle composite values like \"36.8inchx24.4inch\"\n",
    "    parts = re.split(r'[x×\\(\\)]', value)\n",
    "    \n",
    "    for part in parts:\n",
    "        part = part.strip()\n",
    "        if not part:\n",
    "            continue\n",
    "            \n",
    "        # Pattern: decimal number with optional alphabetic suffix\n",
    "        # Match: \"19.2lb\", \"40inch\", \"120hz\", \"1080p\", \"19.5\", \"120\"\n",
    "        match = re.match(r'^(\\d+(?:\\.\\d+)?)[a-z]*$', part)\n",
    "        if match:\n",
    "            # Extract only the numeric part (remove non-numeric suffix)\n",
    "            numeric_part = match.group(1)\n",
    "            model_words.add(numeric_part)\n",
    "    \n",
    "    return model_words\n",
    "\n",
    "\n",
    "def extract_all_model_words_paper_method(item):\n",
    "    \"\"\"\n",
    "    Extract model words from brands, title, and the number 1080.\n",
    "    \n",
    "    1. MW_title: Extract from title only\n",
    "    2. MW_brand: Extract brand name as a model word\n",
    "    3. MW_1080: Extract \"1080\" if it appears in the title\n",
    "    \n",
    "    Returns union of all model words.\n",
    "    \"\"\"\n",
    "    mw_title = set()\n",
    "    mw_brand = set()\n",
    "    mw_1080 = set()\n",
    "    \n",
    "    # Step 1: Extract MW_title from title\n",
    "    title = item.get('title', '') or ''\n",
    "    mw_title.update(extract_model_words_from_title(title))\n",
    "    \n",
    "    # Step 2: Extract brand as a model word\n",
    "    # Brand is critical for matching - products with same brand are more likely duplicates\n",
    "    brand_list = [\n",
    "        \"samsung\", \"lg\", \"sony\", \"philips\", \"panasonic\", \"sharp\", \"toshiba\",\n",
    "        \"vizio\", \"hisense\", \"tcl\", \"insignia\", \"sanyo\", \"magnavox\", \"westinghouse\",\n",
    "        \"element\", \"rca\", \"haier\", \"funai\", \"sceptre\", \"proscan\", \"coby\",\n",
    "        \"supersonic\", \"naxa\", \"dynex\", \"emerson\", \"sylvania\", \"jvc\", \"hitachi\"\n",
    "    ]\n",
    "    \n",
    "    # Check title for brand\n",
    "    title_lower = title.lower()\n",
    "    for brand in brand_list:\n",
    "        if brand in title_lower:\n",
    "            mw_brand.add(brand)\n",
    "            break  # Only add first matching brand\n",
    "    \n",
    "    # Step 3: Extract \"1080\" from title if present\n",
    "    # Look for \"1080\" as a standalone number or as part of \"1080p\"\n",
    "    if re.search(r'1080', title_lower):\n",
    "        mw_1080.add('1080')\n",
    "    \n",
    "    # Return union of all sets (title, brand, and 1080 model words)\n",
    "    # Filter out pure numeric strings (except 1080 which we want to keep)\n",
    "    all_mw = mw_title | mw_brand | mw_1080\n",
    "    # Keep 1080 but filter out other pure numeric strings\n",
    "    filtered_mw = [mw for mw in all_mw if mw == '1080' or not re.match(r'^\\d+(\\.\\d+)?$', mw)]\n",
    "    return filtered_mw\n",
    "\n",
    "\n",
    "def extract_model_words_title_only(item):\n",
    "    \"\"\"\n",
    "    Extract model words ONLY from title (no brand, no 1080).\n",
    "    This is for comparison with the full method.\n",
    "    \n",
    "    Returns only model words extracted from the title.\n",
    "    \"\"\"\n",
    "    mw_title = set()\n",
    "    \n",
    "    # Extract MW_title from title only\n",
    "    title = item.get('title', '') or ''\n",
    "    mw_title.update(extract_model_words_from_title(title))\n",
    "    \n",
    "    # Return only title model words (no brand, no 1080)\n",
    "    # Filter out pure numeric strings\n",
    "    filtered_mw = [mw for mw in mw_title if not re.match(r'^\\d+(\\.\\d+)?$', mw)]\n",
    "    return filtered_mw\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "475a0c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_binary_vectors(titles_model_words, vocab, word_to_index):\n",
    "    binary_vectors = []\n",
    "    for words in titles_model_words:\n",
    "        vector = [0] * len(vocab)\n",
    "        for word in words:\n",
    "            if word in word_to_index:\n",
    "                vector[word_to_index[word]] = 1\n",
    "        binary_vectors.append(vector)\n",
    "    \n",
    "    P = np.array(binary_vectors)\n",
    "    print(f\"Binary vectors shape: {P.shape}\")\n",
    "    return P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6066e73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_prime(n):\n",
    "    if n <= 2:\n",
    "        return 2\n",
    "    n = n if n % 2 != 0 else n + 1  # Make odd\n",
    "    while True:\n",
    "        is_prime = True\n",
    "        for i in range(3, int(n**0.5) + 1, 2):\n",
    "            if n % i == 0:\n",
    "                is_prime = False\n",
    "                break\n",
    "        if is_prime:\n",
    "            return n\n",
    "        n += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2c082d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_minhash_signatures(P, n_hash_functions=None):\n",
    "    num_products, vocab_size = P.shape\n",
    "    \n",
    "    if n_hash_functions is None:\n",
    "        n = int(0.4* vocab_size)  # Increased from 0.4 to 0.5\n",
    "        n = max(n, 2)\n",
    "    \n",
    "    # Choose large prime p > vocab_size\n",
    "    p = next_prime(vocab_size * vocab_size + 1)  # Ensure p > vocab_size^2\n",
    "    m = vocab_size\n",
    "    \n",
    "    print(f\"Using p={p}, hash_functions={n}, vocab_size={vocab_size}\")\n",
    "    \n",
    "    # Generate a,b pairs for n hash functions\n",
    "    a_params = np.random.randint(1, p, size=n)\n",
    "    b_params = np.random.randint(0, p, size=n)\n",
    "    \n",
    "    # OPTIMIZATION: Use full prime range instead of vocab_size to reduce collisions\n",
    "    max_hash_value = p - 1\n",
    "    S = np.full((n, num_products), max_hash_value, dtype=np.int64)\n",
    "    \n",
    "    non_zero_positions = [np.where(P[v] == 1)[0] for v in range(num_products)]\n",
    "    \n",
    "    for i in range(n):\n",
    "        a, b = a_params[i], b_params[i]\n",
    "        for v in range(num_products):\n",
    "            product_set = non_zero_positions[v]\n",
    "            \n",
    "            if len(product_set) > 0:\n",
    "                # Use only % p which gives range [0, p-1] instead of [0, vocab_size-1]\n",
    "                hashes = ((a * product_set + b) % p)\n",
    "                min_hash = np.min(hashes)\n",
    "                S[i, v] = min_hash\n",
    "            \n",
    "    S_normalized = (S % vocab_size).astype(int)\n",
    "    \n",
    "    print(f\"Signature matrix shape: {S_normalized.shape}\")\n",
    "    return S_normalized, (a_params, b_params, p, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d624a0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_true_duplicates_in_candidates(dict_data, candidate_pairs, return_set=False):\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # Build ground truth clusters from modelID\n",
    "    modelID_to_indices = defaultdict(list)\n",
    "    for idx, item in dict_data.items():\n",
    "        if 'modelID' in item:\n",
    "            modelID_to_indices[item['modelID']].append(idx)\n",
    "    \n",
    "    # Keep only modelIDs with 2+ products (actual duplicates)\n",
    "    duplicate_clusters = {mid: set(idxs) for mid, idxs in modelID_to_indices.items() if len(idxs) > 1}\n",
    "    \n",
    "    # Find true duplicates within candidate_pairs\n",
    "    true_duplicates_in_candidates = set()\n",
    "    for i, j in candidate_pairs:\n",
    "        a, b = sorted((i, j))\n",
    "        for cluster in duplicate_clusters.values():\n",
    "            if a in cluster and b in cluster:\n",
    "                true_duplicates_in_candidates.add((a, b))\n",
    "                break\n",
    "    \n",
    "    n_true_duplicates = len(true_duplicates_in_candidates)\n",
    "    \n",
    "    if return_set:\n",
    "        return n_true_duplicates, true_duplicates_in_candidates\n",
    "    else:\n",
    "        return n_true_duplicates\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# n_true = count_true_duplicates_in_candidates(dict_test, candidate_pairs)\n",
    "# n_true, true_set = count_true_duplicates_in_candidates(dict_test, candidate_pairs, return_set=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8bc5a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FUNCTION: Find Best Threshold Using F1 Score on Training Set\n",
    "# ============================================================================\n",
    "def find_best_threshold_on_training_set(dict_train, model_words_train, vocab, word_to_index, \n",
    "                                        thresholds=[0.4, 0.5], f1_range=(0.0, 0.2), verbose=True):\n",
    "    \n",
    "    from collections import defaultdict\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"FINDING BEST THRESHOLD ON TRAINING SET\")\n",
    "        print(\"=\"*70)\n",
    "        f1_max = f1_range[1] if isinstance(f1_range, tuple) else f1_range\n",
    "        print(f\"F1 score upper bound constraint: F1 <= {f1_max:.2f}\")\n",
    "        print(f\"Testing thresholds: {thresholds}\")\n",
    "        print(f\"Training set size: {len(dict_train)} products\")\n",
    "    \n",
    "    # Build ground truth for training set\n",
    "    modelID_to_indices_train = defaultdict(list)\n",
    "    for idx, item in dict_train.items():\n",
    "        if 'modelID' in item:\n",
    "            modelID_to_indices_train[item['modelID']].append(idx)\n",
    "    \n",
    "    # Keep only modelIDs with 2+ products (actual duplicates)\n",
    "    duplicate_clusters_train = {mid: set(idxs) for mid, idxs in modelID_to_indices_train.items() if len(idxs) > 1}\n",
    "    \n",
    "    # Calculate total true duplicate pairs in training set\n",
    "    total_true_pairs_train = sum(len(idxs) * (len(idxs) - 1) // 2 for idxs in duplicate_clusters_train.values())\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Total true duplicate pairs in training set: {total_true_pairs_train}\")\n",
    "        print(f\"Duplicate clusters: {len(duplicate_clusters_train)}\")\n",
    "        print(\"\\nTesting thresholds...\")\n",
    "        print(f\"{'Threshold':<12} {'Candidates':<12} {'TP':<6} {'PQ':<10} {'PC':<10} {'F1':<10}\")\n",
    "        print(\"-\" * 70)\n",
    "    \n",
    "    # Test each threshold\n",
    "    threshold_results = {}\n",
    "    best_threshold = None\n",
    "    best_f1 = -1\n",
    "    \n",
    "    # Create binary vectors and MinHash signatures once (same for all thresholds)\n",
    "    A_train = create_binary_vectors(model_words_train, vocab, word_to_index)\n",
    "    L_train, _ = compute_minhash_signatures(A_train)\n",
    "    product_ids_train = list(dict_train.keys())\n",
    "    n_hash_functions = L_train.shape[0]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Signature matrix shape: {L_train.shape}\")\n",
    "        print(f\"Number of hash functions: {n_hash_functions}\")\n",
    "    \n",
    "    for t in thresholds:\n",
    "        if verbose:\n",
    "            print(f\"\\nTesting threshold {t}...\")\n",
    "        \n",
    "        # Calculate (b, r) for this threshold using theoretical formula: t = (1/b)^(1/r)\n",
    "        # Subject to constraint: b * r == n_hash_functions\n",
    "        best_b, best_r = optimize_lsh_parameters(n_hash_functions, t, min_prob=0.5)\n",
    "        \n",
    "        if verbose:\n",
    "            t_achieved = (1 / best_b) ** (1 / best_r)\n",
    "            print(f\"    Theoretical (b, r) = ({best_b}, {best_r}) for threshold {t}\")\n",
    "            print(f\"    Achieved threshold: {t_achieved:.6f} (error: {abs(t - t_achieved):.6f})\")\n",
    "        \n",
    "        # Run LSH with theoretical (b, r) for this threshold\n",
    "        candidate_pairs = lsh_banding(L_train, product_ids_train, t=t, b=best_b, r=best_r, verbose=False)\n",
    "        \n",
    "        # Count true duplicates using the function\n",
    "        TP = count_true_duplicates_in_candidates(dict_train, candidate_pairs)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        PQ = TP / len(candidate_pairs) if len(candidate_pairs) > 0 else 0.0\n",
    "        PC = TP / total_true_pairs_train if total_true_pairs_train > 0 else 0.0\n",
    "        F1 = 2 * (PQ * PC) / (PQ + PC) if (PQ + PC) > 0 else 0.0\n",
    "        \n",
    "        threshold_results[t] = {\n",
    "            'candidate_pairs': len(candidate_pairs),\n",
    "            'TP': TP,\n",
    "            'PQ': PQ,\n",
    "            'PC': PC,\n",
    "            'F1': F1,\n",
    "            'best_b': best_b,\n",
    "            'best_r': best_r\n",
    "        }\n",
    "        \n",
    "        if verbose:\n",
    "            f1_max = f1_range[1] if isinstance(f1_range, tuple) else f1_range\n",
    "            f1_status = \"✓\" if F1 <= f1_max else \"✗\"\n",
    "            print(f\"{t:<12.2f} {len(candidate_pairs):<12} {TP:<6} {PQ:<10.6f} {PC:<10.6f} {F1:<10.6f} (b={best_b}, r={best_r}) {f1_status}\")\n",
    "        \n",
    "        # Update best threshold (only if F1 <= f1_max)\n",
    "        f1_max = f1_range[1] if isinstance(f1_range, tuple) else f1_range\n",
    "        if F1 <= f1_max and F1 > best_f1:\n",
    "            best_f1 = F1\n",
    "            best_threshold = t\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"-\" * 70)\n",
    "        if best_threshold is not None:\n",
    "            f1_max = f1_range[1] if isinstance(f1_range, tuple) else f1_range\n",
    "            if best_f1 <= f1_max:\n",
    "                print(f\"\\nBest threshold: {best_threshold} (F1 = {best_f1:.6f}) [F1 <= {f1_max:.2f} ✓]\")\n",
    "            else:\n",
    "                print(f\"\\nBest threshold: {best_threshold} (F1 = {best_f1:.6f}) [F1 > {f1_max:.2f} ✗ - no threshold found with F1 <= {f1_max:.2f}]\")\n",
    "        else:\n",
    "            print(f\"\\nNo valid threshold found!\")\n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    return best_threshold, best_f1, threshold_results\n",
    "\n",
    "def find_best_lsh_bands_rows_for_threshold(L_train, product_ids_train, dict_train, threshold, \n",
    "                                          n_hash_functions, max_configs=30, f1_range=(0.0, 0.2), verbose=True):\n",
    "   \n",
    "    from collections import defaultdict\n",
    "    import numpy as np\n",
    "    \n",
    "    # Build ground truth for training set\n",
    "    modelID_to_indices_train = defaultdict(list)\n",
    "    for idx, item in dict_train.items():\n",
    "        if 'modelID' in item:\n",
    "            modelID_to_indices_train[item['modelID']].append(idx)\n",
    "    \n",
    "    duplicate_clusters_train = {mid: set(idxs) for mid, idxs in modelID_to_indices_train.items() if len(idxs) > 1}\n",
    "    total_true_pairs_train = sum(len(idxs) * (len(idxs) - 1) // 2 for idxs in duplicate_clusters_train.values())\n",
    "    \n",
    "    # Generate candidate (b, r) pairs\n",
    "    # Constraint: b * r == n_hash_functions (STRICT EQUALITY)\n",
    "    configs = []\n",
    "    min_r = 2\n",
    "    max_r = n_hash_functions##min(int(np.sqrt(n_hash_functions)) + 10, n_hash_functions // 2)\n",
    "    \n",
    "    for r in range(min_r, max_r + 1):\n",
    "        if n_hash_functions % r == 0:  # Only if r divides n_hash_functions exactly\n",
    "            b = n_hash_functions // r\n",
    "            if b >= 1 and b * r == n_hash_functions:  # STRICT equality: b * r == n\n",
    "                configs.append((b, r))\n",
    "    \n",
    "    # Limit number of configurations to test\n",
    "    if len(configs) > max_configs:\n",
    "        # Sample evenly across the range\n",
    "        step = len(configs) // max_configs\n",
    "        configs = configs[::max(1, step)][:max_configs]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"    Testing {len(configs)} (b, r) configurations for threshold {threshold}...\")\n",
    "    \n",
    "    best_b, best_r, best_f1 = None, None, -1\n",
    "    config_results = {}\n",
    "    \n",
    "    for b, r in configs:\n",
    "        # Run LSH with this configuration\n",
    "        candidate_pairs = lsh_banding(L_train, product_ids_train, t=threshold, b=b, r=r, verbose=False)\n",
    "        \n",
    "        # Evaluate\n",
    "        TP = count_true_duplicates_in_candidates(dict_train, candidate_pairs)\n",
    "        PQ = TP / len(candidate_pairs) if len(candidate_pairs) > 0 else 0.0\n",
    "        PC = TP / total_true_pairs_train if total_true_pairs_train > 0 else 0.0\n",
    "        F1 = 2 * (PQ * PC) / (PQ + PC) if (PQ + PC) > 0 else 0.0\n",
    "        \n",
    "        config_results[(b, r)] = {\n",
    "            'candidate_pairs': len(candidate_pairs),\n",
    "            'TP': TP,\n",
    "            'PQ': PQ,\n",
    "            'PC': PC,\n",
    "            'F1': F1\n",
    "        }\n",
    "        \n",
    "        # Only consider configurations with F1 <= f1_max (upper bound constraint)\n",
    "        # Select the best F1 that satisfies F1 <= f1_max\n",
    "        f1_max = f1_range[1] if isinstance(f1_range, tuple) else f1_range\n",
    "        if F1 <= f1_max and F1 > best_f1:\n",
    "            best_f1 = F1\n",
    "            best_b = b\n",
    "            best_r = r\n",
    "    \n",
    "    if verbose:\n",
    "        if best_b is not None:\n",
    "            f1_max = f1_range[1] if isinstance(f1_range, tuple) else f1_range\n",
    "            if best_f1 <= f1_max:\n",
    "                print(f\"      Best (b, r) = ({best_b}, {best_r}) with F1 = {best_f1:.6f} [F1 <= {f1_max:.2f} ✓]\")\n",
    "            else:\n",
    "                print(f\"      Best (b, r) = ({best_b}, {best_r}) with F1 = {best_f1:.6f} [F1 > {f1_max:.2f} ✗]\")\n",
    "        else:\n",
    "            f1_max = f1_range[1] if isinstance(f1_range, tuple) else f1_range\n",
    "            print(f\"      No valid (b, r) configuration found with F1 <= {f1_max:.2f}!\")\n",
    "    \n",
    "    return best_b, best_r, best_f1, config_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b8a03ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "####new\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "\n",
    "def lsh_banding(M, product_ids, t=0.5, b=None, r=None, verbose=True):\n",
    "    n_rows, num_products = M.shape\n",
    "    \n",
    "    # Optimize b and r if not provided\n",
    "    if b is None or r is None:\n",
    "        b, r = optimize_lsh_parameters(n_rows, t)\n",
    "    \n",
    "    # Validate constraint: b * r == n_rows (STRICT)\n",
    "    if b * r != n_rows:\n",
    "        raise ValueError(f\"LSH constraint violation: b * r ({b} * {r} = {b*r}) must equal n_rows ({n_rows})\")\n",
    "    \n",
    "    if verbose:\n",
    "        prob_at_t = 1 - (1 - t**r)**b\n",
    "    \n",
    "    candidate_pairs = set()\n",
    "    \n",
    "    # Process each band\n",
    "    # Since b * r == n_rows, end_row will always be start_row + r (no need for min)\n",
    "    for band_idx in range(b):\n",
    "        start_row = band_idx * r\n",
    "        end_row = start_row + r  # Since b * r == n_rows, this is always <= n_rows\n",
    "        \n",
    "        # Extract band signatures\n",
    "        band_matrix = M[start_row:end_row, :]\n",
    "        \n",
    "        # Hash products into buckets\n",
    "        buckets = defaultdict(list)\n",
    "        \n",
    "        for prod_idx in range(num_products):\n",
    "            # Get signature for this product in this band\n",
    "            signature = tuple(band_matrix[:, prod_idx].tolist())\n",
    "            \n",
    "            # Hash the signature (use band index for independence)\n",
    "            h = hashlib.md5()\n",
    "            h.update(str(band_idx).encode())\n",
    "            h.update(str(signature).encode())\n",
    "            bucket_hash = h.hexdigest()\n",
    "            \n",
    "            buckets[bucket_hash].append(prod_idx)\n",
    "        \n",
    "        # Generate candidate pairs from buckets with 2+ products\n",
    "        for bucket_products in buckets.values():\n",
    "            if len(bucket_products) >= 2:\n",
    "                for i in range(len(bucket_products)):\n",
    "                    for j in range(i + 1, len(bucket_products)):\n",
    "                        idx_i = bucket_products[i]\n",
    "                        idx_j = bucket_products[j]\n",
    "                        pair = tuple(sorted([product_ids[idx_i], product_ids[idx_j]]))\n",
    "                        candidate_pairs.add(pair)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Candidate pairs: {len(candidate_pairs)}\")\n",
    "        max_possible = num_products * (num_products - 1) // 2\n",
    "        print(f\"  vs. All possible pairs: {max_possible}\")\n",
    "        print(f\"  Reduction: {100 * (1 - len(candidate_pairs) / max_possible):.1f}%\")\n",
    "    \n",
    "    return candidate_pairs\n",
    "\n",
    "\n",
    "def optimize_lsh_parameters(n, t, min_prob=0.5):\n",
    "    \"\"\"\n",
    "    Optimize b (bands) and r (rows per band) for given threshold t.\n",
    "    \n",
    "    Constraints:\n",
    "    - ENFORCES: b * r == n (strict equality)\n",
    "    - APPROXIMATES: t = (1/b)^(1/r) (minimizes error from desired threshold)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    best_b, best_r = None, None\n",
    "    best_error = float('inf')\n",
    "    \n",
    "    # Try all possible r values\n",
    "    for r in range(1, min(n + 1, 100)):  # Increased range for better optimization\n",
    "        # Calculate b from the LSH equation: t = (1/b)^(1/r)\n",
    "        # Rearranging: b = 1/(t^r)\n",
    "        b_from_equation = 1 / (t ** r)\n",
    "        \n",
    "        # ENFORCE: b * r == n (STRICT EQUALITY)\n",
    "        # Only consider r values that divide n exactly\n",
    "        if n % r != 0:\n",
    "            continue\n",
    "        \n",
    "        b = n // r  # This ensures b * r == n exactly\n",
    "        if b < 1:\n",
    "            continue\n",
    "        \n",
    "        # APPROXIMATE: t = (1/b)^(1/r) (minimize error from desired threshold)\n",
    "        # Calculate the threshold that would be achieved with this (b, r)\n",
    "        t_achieved = (1 / b) ** (1 / r)\n",
    "        \n",
    "        # Calculate error: how far is t_achieved from desired t?\n",
    "        error = abs(t - t_achieved)\n",
    "        \n",
    "        b_final = b\n",
    "        t_final = t_achieved\n",
    "        error_final = error\n",
    "        \n",
    "        # Calculate probability at threshold for validation\n",
    "        prob_at_t = 1 - (1 - t_final**r)**b_final\n",
    "        \n",
    "        # Only consider configurations that meet minimum probability requirement\n",
    "        if prob_at_t < min_prob:\n",
    "            continue\n",
    "        \n",
    "        # Select configuration with smallest error from desired threshold\n",
    "        if error_final < best_error:\n",
    "            best_error = error_final\n",
    "            best_b = b_final\n",
    "            best_r = r\n",
    "    \n",
    "    # Fallback if no valid configuration found\n",
    "    # Still enforce b * r == n strictly\n",
    "    if best_b is None:\n",
    "        # Find r that divides n and gives best approximation to threshold\n",
    "        best_error_fallback = float('inf')\n",
    "        for r in range(1, min(n + 1, 100)):\n",
    "            if n % r != 0:  # Must divide n exactly\n",
    "                continue\n",
    "            b = n // r\n",
    "            if b < 1:\n",
    "                continue\n",
    "            t_calc = (1 / b) ** (1 / r)\n",
    "            error_calc = abs(t - t_calc)\n",
    "            if error_calc < best_error_fallback:\n",
    "                best_error_fallback = error_calc\n",
    "                best_r = r\n",
    "                best_b = b\n",
    "    \n",
    "    # Final fallback: if still no valid configuration, use r=1, b=n (guaranteed to work)\n",
    "    if best_b is None:\n",
    "        best_r = 1\n",
    "        best_b = n\n",
    "        if verbose:\n",
    "            print(f\"WARNING: No optimal (b, r) found, using fallback: b={best_b}, r={best_r}\")\n",
    "    \n",
    "    return best_b, best_r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7995f95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AGGLOMERATIVE CLUSTERING FUNCTIONS\n",
    "# ============================================================================\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def compute_tmwm_similarity(item1, item2):\n",
    "    \"\"\"\n",
    "    Compute TMWM (Title Model Words Method) similarity between two products.\n",
    "    Uses overlap coefficient on model word sets: |A ∩ B| / min(|A|, |B|)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    item1 : dict\n",
    "        First product item (must have 'title' key)\n",
    "    item2 : dict\n",
    "        Second product item (must have 'title' key)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    similarity : float\n",
    "        TMWM similarity between 0.0 and 1.0\n",
    "    \"\"\"\n",
    "    # Extract model words from titles\n",
    "    title1 = item1.get('title', '') or ''\n",
    "    title2 = item2.get('title', '') or ''\n",
    "    \n",
    "    # Extract model words using the existing function\n",
    "    mw1 = extract_model_words_from_title(title1)\n",
    "    mw2 = extract_model_words_from_title(title2)\n",
    "    \n",
    "    # If either has no model words, return 0 similarity\n",
    "    if not mw1 or not mw2:\n",
    "        return 0.0\n",
    "    \n",
    "    # Compute overlap coefficient: intersection / min(size)\n",
    "    intersection = len(mw1 & mw2)\n",
    "    min_size = min(len(mw1), len(mw2))\n",
    "    \n",
    "    if min_size == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    similarity = intersection / min_size\n",
    "    return min(1.0, similarity)  # Ensure it's between 0 and 1\n",
    "\n",
    "\n",
    "def build_similarity_matrix_from_pairs(filtered_pairs, all_product_ids, dict_data):\n",
    "    \"\"\"\n",
    "    Build a similarity matrix from filtered pairs using TMWM similarity.\n",
    "    Products in filtered_pairs get their actual TMWM similarity score,\n",
    "    others get 0.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filtered_pairs : set of tuples\n",
    "        Set of (product_id1, product_id2) pairs\n",
    "    all_product_ids : list\n",
    "        List of all product IDs to include in the matrix\n",
    "    dict_data : dict\n",
    "        Dictionary mapping product IDs to product data (must have 'title' key)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    similarity_matrix : np.ndarray\n",
    "        TMWM similarity matrix (values between 0.0 and 1.0)\n",
    "    product_id_to_index : dict\n",
    "        Mapping from product ID to matrix index\n",
    "    \"\"\"\n",
    "    n = len(all_product_ids)\n",
    "    product_id_to_index = {pid: idx for idx, pid in enumerate(all_product_ids)}\n",
    "    \n",
    "    # Initialize similarity matrix (all zeros)\n",
    "    similarity_matrix = np.zeros((n, n), dtype=float)\n",
    "    \n",
    "    # Set diagonal to 1.0 (self-similarity)\n",
    "    np.fill_diagonal(similarity_matrix, 1.0)\n",
    "    \n",
    "    # Compute TMWM similarity for each pair\n",
    "    computed = 0\n",
    "    for i, j in filtered_pairs:\n",
    "        if i in product_id_to_index and j in product_id_to_index:\n",
    "            idx_i = product_id_to_index[i]\n",
    "            idx_j = product_id_to_index[j]\n",
    "            \n",
    "            # Get product data\n",
    "            item_i = dict_data.get(i)\n",
    "            item_j = dict_data.get(j)\n",
    "            \n",
    "            if item_i and item_j:\n",
    "                # Compute TMWM similarity\n",
    "                tmwm_sim = compute_tmwm_similarity(item_i, item_j)\n",
    "                similarity_matrix[idx_i, idx_j] = tmwm_sim\n",
    "                similarity_matrix[idx_j, idx_i] = tmwm_sim  # Symmetric\n",
    "                computed += 1\n",
    "    \n",
    "    print(f\"  Computed TMWM similarity for {computed} pairs\")\n",
    "    \n",
    "    return similarity_matrix, product_id_to_index\n",
    "\n",
    "\n",
    "def apply_agglomerative_clustering(filtered_pairs, all_product_ids, dict_data,\n",
    "                                   linkage='complete', n_clusters=None, \n",
    "                                   distance_threshold=None):\n",
    "    \"\"\"\n",
    "    Apply Agglomerative Clustering to filtered pairs using TMWM similarity.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filtered_pairs : set of tuples\n",
    "        Set of (product_id1, product_id2) pairs\n",
    "    all_product_ids : list\n",
    "        List of all product IDs\n",
    "    dict_data : dict\n",
    "        Dictionary mapping product IDs to product data (must have 'title' key)\n",
    "    linkage : str\n",
    "        Linkage criterion ('complete', 'single', 'average', 'ward')\n",
    "        'complete' = maximum linkage (double linkage)\n",
    "    n_clusters : int, optional\n",
    "        Number of clusters to form\n",
    "    distance_threshold : float, optional\n",
    "        Distance threshold for cutting the tree\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    cluster_labels : np.ndarray\n",
    "        Cluster labels for each product\n",
    "    clustering_model : AgglomerativeClustering\n",
    "        The fitted clustering model\n",
    "    \"\"\"\n",
    "    if len(filtered_pairs) == 0:\n",
    "        # If no pairs, each product is its own cluster\n",
    "        return np.zeros(len(all_product_ids)), None\n",
    "    \n",
    "    # Build similarity matrix using TMWM\n",
    "    similarity_matrix, product_id_to_index = build_similarity_matrix_from_pairs(\n",
    "        filtered_pairs, all_product_ids, dict_data\n",
    "    )\n",
    "    \n",
    "    # Convert similarity to distance (for clustering, we need distance)\n",
    "    # Similarity 1 -> distance 0, similarity 0 -> distance 1\n",
    "    distance_matrix = 1.0 - similarity_matrix\n",
    "    \n",
    "    # Apply clustering\n",
    "    # If n_clusters is None, use distance_threshold\n",
    "    clustering = AgglomerativeClustering(\n",
    "        n_clusters=n_clusters,\n",
    "        metric='precomputed',\n",
    "        linkage=linkage,\n",
    "        distance_threshold=distance_threshold,\n",
    "        compute_full_tree=True\n",
    "    )\n",
    "    \n",
    "    cluster_labels = clustering.fit_predict(distance_matrix)\n",
    "    \n",
    "    return cluster_labels, clustering\n",
    "\n",
    "\n",
    "def evaluate_clustering_f1_score(cluster_labels, dict_data, product_ids):\n",
    "    \"\"\"\n",
    "    Evaluate clustering results using F1 score based on ground truth modelID.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cluster_labels : np.ndarray\n",
    "        Cluster labels from clustering\n",
    "    dict_data : dict\n",
    "        Dictionary mapping product IDs to product data\n",
    "    product_ids : list\n",
    "        List of product IDs in the same order as cluster_labels\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    f1_score : float\n",
    "        F1 score for the clustering\n",
    "    precision : float\n",
    "        Precision (Pair Quality)\n",
    "    recall : float\n",
    "        Recall (Pair Completeness)\n",
    "    \"\"\"\n",
    "    # Build ground truth clusters from modelID\n",
    "    modelID_to_indices = defaultdict(list)\n",
    "    for idx, product_id in enumerate(product_ids):\n",
    "        item = dict_data.get(product_id)\n",
    "        if item and 'modelID' in item:\n",
    "            modelID_to_indices[item['modelID']].append(idx)\n",
    "    \n",
    "    # Keep only modelIDs with 2+ products (actual duplicates)\n",
    "    true_clusters = {mid: set(idxs) for mid, idxs in modelID_to_indices.items() if len(idxs) > 1}\n",
    "    \n",
    "    # Build predicted clusters from cluster labels\n",
    "    predicted_clusters = defaultdict(set)\n",
    "    for idx, label in enumerate(cluster_labels):\n",
    "        predicted_clusters[label].add(idx)\n",
    "    \n",
    "    # Remove singleton clusters (they don't contribute to pairs)\n",
    "    predicted_clusters = {label: cluster for label, cluster in predicted_clusters.items() if len(cluster) > 1}\n",
    "    \n",
    "    # Calculate true positive pairs (pairs that are in both true and predicted clusters)\n",
    "    true_positive_pairs = set()\n",
    "    \n",
    "    # For each predicted cluster\n",
    "    for pred_cluster in predicted_clusters.values():\n",
    "        # Check if all products in this cluster have the same modelID\n",
    "        model_ids_in_cluster = set()\n",
    "        for idx in pred_cluster:\n",
    "            product_id = product_ids[idx]\n",
    "            item = dict_data.get(product_id)\n",
    "            if item and 'modelID' in item:\n",
    "                model_ids_in_cluster.add(item['modelID'])\n",
    "        \n",
    "        # If all products have the same modelID, all pairs are true positives\n",
    "        if len(model_ids_in_cluster) == 1:\n",
    "            model_id = list(model_ids_in_cluster)[0]\n",
    "            if model_id in true_clusters:\n",
    "                # All pairs within this cluster are true positives\n",
    "                cluster_list = sorted(list(pred_cluster))\n",
    "                for i in range(len(cluster_list)):\n",
    "                    for j in range(i + 1, len(cluster_list)):\n",
    "                        true_positive_pairs.add((cluster_list[i], cluster_list[j]))\n",
    "    \n",
    "    # Calculate total true pairs\n",
    "    total_true_pairs = sum(len(idxs) * (len(idxs) - 1) // 2 for idxs in true_clusters.values())\n",
    "    \n",
    "    # Calculate total predicted pairs\n",
    "    total_predicted_pairs = sum(len(cluster) * (len(cluster) - 1) // 2 for cluster in predicted_clusters.values())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    TP = len(true_positive_pairs)\n",
    "    precision = TP / total_predicted_pairs if total_predicted_pairs > 0 else 0.0\n",
    "    recall = TP / total_true_pairs if total_true_pairs > 0 else 0.0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return f1_score, precision, recall, TP, total_predicted_pairs, total_true_pairs\n",
    "\n",
    "\n",
    "def find_best_clustering_distance_threshold(filtered_pairs_train, all_product_ids_train, dict_train,\n",
    "                                            linkage='complete',\n",
    "                                            distance_thresholds=[0.2, 0.3, 0.4, 0.5, 0.6, 0.7],\n",
    "                                            f1_range=(0.1, 0.2),\n",
    "                                            verbose=True):\n",
    "    \"\"\"\n",
    "    Find the best distance_threshold for agglomerative clustering on training set.\n",
    "    Includes safeguards against overfitting by preferring thresholds within F1 range.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filtered_pairs_train : set of tuples\n",
    "        Training set filtered pairs\n",
    "    all_product_ids_train : list\n",
    "        List of all training product IDs\n",
    "    dict_train : dict\n",
    "        Training data dictionary\n",
    "    linkage : str\n",
    "        Linkage criterion (default: 'complete')\n",
    "    distance_thresholds : list\n",
    "        List of distance thresholds to test\n",
    "    f1_range : tuple\n",
    "        Desired F1 range (min, max) - only consider thresholds in this range\n",
    "    verbose : bool\n",
    "        Whether to print progress\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    best_threshold : float\n",
    "        Best distance threshold\n",
    "    best_f1_train : float\n",
    "        Best F1 score on training set\n",
    "    threshold_results : dict\n",
    "        Results for all tested thresholds\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"FINDING BEST CLUSTERING DISTANCE THRESHOLD ON TRAINING SET\")\n",
    "        print(\"=\"*70)\n",
    "        f1_max = f1_range[1] if isinstance(f1_range, tuple) else f1_range\n",
    "        print(f\"F1 score upper bound constraint: F1 <= {f1_max:.2f}\")\n",
    "        print(f\"Testing distance thresholds: {distance_thresholds}\")\n",
    "        print(f\"Linkage: {linkage}\")\n",
    "        print(f\"Training set size: {len(all_product_ids_train)} products\")\n",
    "        print(f\"Filtered pairs: {len(filtered_pairs_train)}\")\n",
    "    \n",
    "    # Build ground truth for training set\n",
    "    modelID_to_indices_train = defaultdict(list)\n",
    "    for idx, product_id in enumerate(all_product_ids_train):\n",
    "        item = dict_train.get(product_id)\n",
    "        if item and 'modelID' in item:\n",
    "            modelID_to_indices_train[item['modelID']].append(idx)\n",
    "    \n",
    "    duplicate_clusters_train = {mid: set(idxs) for mid, idxs in modelID_to_indices_train.items() if len(idxs) > 1}\n",
    "    total_true_pairs_train = sum(len(idxs) * (len(idxs) - 1) // 2 for idxs in duplicate_clusters_train.values())\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Total true duplicate pairs in training set: {total_true_pairs_train}\")\n",
    "        print(f\"Duplicate clusters: {len(duplicate_clusters_train)}\")\n",
    "        print(\"\\nTesting thresholds...\")\n",
    "        print(f\"{'Threshold':<12} {'Clusters':<10} {'TP':<6} {'PQ':<10} {'PC':<10} {'F1':<10} {'Status':<10}\")\n",
    "        print(\"-\" * 70)\n",
    "    \n",
    "    threshold_results = {}\n",
    "    best_threshold = None\n",
    "    best_f1 = -1\n",
    "    best_in_range = None\n",
    "    best_f1_in_range = -1\n",
    "    \n",
    "    for threshold in distance_thresholds:\n",
    "        # Apply clustering with this threshold\n",
    "        cluster_labels, _ = apply_agglomerative_clustering(\n",
    "            filtered_pairs_train,\n",
    "            all_product_ids_train,\n",
    "            dict_train,\n",
    "            linkage=linkage,\n",
    "            distance_threshold=threshold\n",
    "        )\n",
    "        \n",
    "        # Evaluate clustering\n",
    "        f1, precision, recall, TP, total_predicted_pairs, total_true_pairs = evaluate_clustering_f1_score(\n",
    "            cluster_labels, dict_train, all_product_ids_train\n",
    "        )\n",
    "        \n",
    "        n_clusters = len(set(cluster_labels))\n",
    "        f1_max = f1_range[1] if isinstance(f1_range, tuple) else f1_range\n",
    "        in_range = f1 <= f1_max\n",
    "        \n",
    "        threshold_results[threshold] = {\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'TP': TP,\n",
    "            'total_predicted_pairs': total_predicted_pairs,\n",
    "            'total_true_pairs': total_true_pairs,\n",
    "            'n_clusters': n_clusters,\n",
    "            'in_range': in_range\n",
    "        }\n",
    "        \n",
    "        status = \"✓ IN RANGE\" if in_range else \"✗ OUT\"\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"{threshold:<12.2f} {n_clusters:<10} {TP:<6} {precision:<10.6f} {recall:<10.6f} {f1:<10.6f} {status:<10}\")\n",
    "        \n",
    "        # Track best overall (only if F1 <= f1_max)\n",
    "        f1_max = f1_range[1] if isinstance(f1_range, tuple) else f1_range\n",
    "        if f1 <= f1_max and f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "        \n",
    "        # Track best within range (preferred to avoid overfitting)\n",
    "        if in_range and f1 > best_f1_in_range:\n",
    "            best_f1_in_range = f1\n",
    "            best_in_range = threshold\n",
    "    \n",
    "    # Select best threshold (prefer in-range to avoid overfitting)\n",
    "    f1_max = f1_range[1] if isinstance(f1_range, tuple) else f1_range\n",
    "    if best_in_range is not None:\n",
    "        selected_threshold = best_in_range\n",
    "        selected_f1 = best_f1_in_range\n",
    "        selection_reason = f\"F1 <= {f1_max:.2f} ✓\"\n",
    "    else:\n",
    "        selected_threshold = best_threshold\n",
    "        selected_f1 = best_f1\n",
    "        selection_reason = f\"F1 > {f1_max:.2f} ✗ (no threshold found with F1 <= {f1_max:.2f})\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"-\" * 70)\n",
    "        print(f\"\\nBest threshold: {selected_threshold} (F1 = {selected_f1:.6f}) [{selection_reason}]\")\n",
    "        f1_max = f1_range[1] if isinstance(f1_range, tuple) else f1_range\n",
    "        if best_in_range is None:\n",
    "            print(f\"WARNING: No threshold found with F1 <= {f1_max:.2f}\")\n",
    "            print(f\"Using best available: {best_threshold} (F1 = {best_f1:.6f})\")\n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    return selected_threshold, selected_f1, threshold_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e29d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOP_LIST = [\n",
    "    'newegg', 'amazon', 'bestbuy', 'walmart', 'target', 'costco', \n",
    "    'bhphotovideo', 'tigerdirect', 'overstock', 'sears', 'ebay', 'thenerds'\n",
    "]\n",
    "\n",
    "BRAND_LIST = [\n",
    "    \"samsung\", \"lg\", \"sony\", \"philips\", \"panasonic\", \"sharp\", \"toshiba\",\n",
    "    \"vizio\", \"hisense\", \"tcl\", \"insignia\", \"sanyo\", \"magnavox\", \"westinghouse\",\n",
    "    \"element\", \"rca\", \"haier\", \"funai\", \"sceptre\", \"proscan\", \"coby\",\n",
    "    \"supersonic\", \"naxa\", \"dynex\", \"emerson\", \"sylvania\", \"jvc\", \"hitachi\"\n",
    "]\n",
    "\n",
    "def extract_brand_from_item(item, brand_list=BRAND_LIST):\n",
    "    \"\"\"Extract brand from title or featuresmap\"\"\"\n",
    "    # Check title first\n",
    "    title = item.get('title', '').lower()\n",
    "    for b in brand_list:\n",
    "        if b in title:\n",
    "            return b\n",
    "    \n",
    "    # Check featuresmap\n",
    "    features = item.get('featuresmap') or item.get('featuresMap')\n",
    "    if isinstance(features, dict):\n",
    "        brand_val = str(features.get('brand', '')).lower()\n",
    "        for b in brand_list:\n",
    "            if b in brand_val:\n",
    "                return b\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_shop_from_item(item, shop_list=SHOP_LIST):\n",
    "    \"\"\"Extract shop/website from URL, title, or shop field\"\"\"\n",
    "    # Check shop field first (most reliable)\n",
    "    shop_field = item.get('shop', '') or item.get('Shop', '')\n",
    "    if shop_field and isinstance(shop_field, str):\n",
    "        shop_lower = shop_field.lower()\n",
    "        for shop in shop_list:\n",
    "            if shop in shop_lower:\n",
    "                return shop\n",
    "    \n",
    "    # Check URL field\n",
    "    url = item.get('url') or item.get('URL')\n",
    "    if url and isinstance(url, str):\n",
    "        url_lower = url.lower()\n",
    "        for shop in shop_list:\n",
    "            if shop in url_lower:\n",
    "                return shop\n",
    "    \n",
    "    # Check title for shop names (may be removed in cleaned version)\n",
    "    title = item.get('title', '') or item.get('Title', '')\n",
    "    if title and isinstance(title, str):\n",
    "        title_lower = title.lower()\n",
    "        for shop in shop_list:\n",
    "            if shop in title_lower:\n",
    "                return shop\n",
    "    return None\n",
    "\n",
    "def filter_candidate_pairs_by_brand_and_shop(candidate_pairs, dict_data, \n",
    "                                              brand_list=BRAND_LIST, \n",
    "                                              shop_list=SHOP_LIST,\n",
    "                                              dict_data_with_shops=None,\n",
    "                                              verbose=True):\n",
    "    \n",
    "    filtered_pairs = set()\n",
    "    removed_diff_brand = 0\n",
    "    removed_same_shop = 0\n",
    "    \n",
    "    for i, j in candidate_pairs:\n",
    "        pair_i, pair_j = i, j  # Alias for debug clarity\n",
    "        item_i = dict_data[i]\n",
    "        item_j = dict_data[j]\n",
    "        \n",
    "        # Extract brands\n",
    "        brand_i = extract_brand_from_item(item_i, brand_list)\n",
    "        brand_j = extract_brand_from_item(item_j, brand_list)\n",
    "        \n",
    "        # Rule 1: diffBrand() - Remove if both brands identified AND different\n",
    "        if brand_i is not None and brand_j is not None and brand_i != brand_j:\n",
    "            removed_diff_brand += 1\n",
    "            continue\n",
    "        \n",
    "        # Extract shops\n",
    "        # If dict_data has shops removed, try to get from dict_data_with_shops if provided\n",
    "        if dict_data_with_shops is not None:\n",
    "            shop_item_i = dict_data_with_shops.get(i, item_i)\n",
    "            shop_item_j = dict_data_with_shops.get(j, item_j)\n",
    "        else:\n",
    "            shop_item_i = item_i\n",
    "            shop_item_j = item_j\n",
    "        \n",
    "        shop_i = extract_shop_from_item(shop_item_i, shop_list)\n",
    "        shop_j = extract_shop_from_item(shop_item_j, shop_list)\n",
    "        \n",
    "         # If dict_data has shops removed, try to get from dict_data_with_shops if provided\n",
    "        if dict_data_with_shops is not None:\n",
    "            shop_item_i = dict_data_with_shops.get(i, item_i)\n",
    "            shop_item_j = dict_data_with_shops.get(j, item_j)\n",
    "        else:\n",
    "            shop_item_i = item_i\n",
    "            shop_item_j = item_j\n",
    "        \n",
    "        shop_i = extract_shop_from_item(shop_item_i, shop_list)\n",
    "        shop_j = extract_shop_from_item(shop_item_j, shop_list)\n",
    "        \n",
    "    \n",
    "        # Rule 2: sameShop() - Remove if same shop (duplicates are cross-shop)\n",
    "        if shop_i is not None and shop_j is not None and shop_i == shop_j:\n",
    "            removed_same_shop += 1\n",
    "            continue\n",
    "        \n",
    "        filtered_pairs.add((i, j))\n",
    "    \n",
    "    stats = {\n",
    "        'original': len(candidate_pairs),\n",
    "        'removed_diff_brand': removed_diff_brand,\n",
    "        'removed_same_shop': removed_same_shop,\n",
    "        'filtered': len(filtered_pairs)\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Original candidate pairs: {stats['original']}\")\n",
    "        print(f\"Removed (different brands): {stats['removed_diff_brand']}\")\n",
    "        print(f\"Removed (same shop): {stats['removed_same_shop']}\")\n",
    "        print(f\"Filtered pairs remaining: {stats['filtered']}\")\n",
    "    \n",
    "    return filtered_pairs, stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aebc8f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BOOTSTRAP EVALUATION: 5 Iterations\n",
      "======================================================================\n",
      "Total samples: 1624\n",
      "Number of bootstrap iterations: 5\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "BOOTSTRAP ITERATION 1/5\n",
      "======================================================================\n",
      "\n",
      "--- Bootstrap 1 Split ---\n",
      "  Training set: 1624 samples (1048 unique) -> using 1048 unique products\n",
      "  Test set: 576 samples (576 unique) -> using 576 unique products\n",
      "\n",
      "======================================================================\n",
      "METHOD: CURRENT\n",
      "======================================================================\n",
      "\n",
      "--- Model Word Extraction (current) ---\n",
      "  Extracted model words for 1048 training products\n",
      "  Extracted model words for 576 test products\n",
      "\n",
      "--- Model Word Extraction ---\n",
      "  Extracted model words for 1048 training products\n",
      "  Extracted model words for 576 test products\n",
      "\n",
      "--- Vocabulary Building ---\n",
      "  Training set unique model words: 1680\n",
      "  Test set unique model words: 1113\n",
      "  Combined vocabulary size: 2186\n",
      "\n",
      "======================================================================\n",
      "FINDING BEST THRESHOLD ON TRAINING SET\n",
      "======================================================================\n",
      "F1 score upper bound constraint: F1 <= 0.20\n",
      "Testing thresholds: [0.3, 0.2, 0.4]\n",
      "Training set size: 1048 products\n",
      "Total true duplicate pairs in training set: 169\n",
      "Duplicate clusters: 153\n",
      "\n",
      "Testing thresholds...\n",
      "Threshold    Candidates   TP     PQ         PC         F1        \n",
      "----------------------------------------------------------------------\n",
      "Binary vectors shape: (1048, 2186)\n",
      "Using p=4778603, hash_functions=874, vocab_size=2186\n",
      "Signature matrix shape: (874, 1048)\n",
      "Signature matrix shape: (874, 1048)\n",
      "Number of hash functions: 874\n",
      "\n",
      "Testing threshold 0.3...\n",
      "    Theoretical (b, r) = (437, 2) for threshold 0.3\n",
      "    Achieved threshold: 0.047836 (error: 0.252164)\n",
      "0.30         333471       169    0.000507   1.000000   0.001013   (b=437, r=2) ✓\n",
      "\n",
      "Testing threshold 0.2...\n",
      "    Theoretical (b, r) = (437, 2) for threshold 0.2\n",
      "    Achieved threshold: 0.047836 (error: 0.152164)\n",
      "0.20         333471       169    0.000507   1.000000   0.001013   (b=437, r=2) ✓\n",
      "\n",
      "Testing threshold 0.4...\n",
      "    Theoretical (b, r) = (437, 2) for threshold 0.4\n",
      "    Achieved threshold: 0.047836 (error: 0.352164)\n",
      "0.40         333471       169    0.000507   1.000000   0.001013   (b=437, r=2) ✓\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Best threshold: 0.3 (F1 = 0.001013) [F1 <= 0.20 ✓]\n",
      "======================================================================\n",
      "\n",
      "--- Best Threshold Selection ---\n",
      "  Selected threshold: 0.3 (F1 on training: 0.001013)\n",
      "  Optimal LSH parameters: b=437, r=2\n",
      "Binary vectors shape: (576, 2186)\n",
      "\n",
      "--- Binary Vector Creation ---\n",
      "  Test set binary vectors shape: (576, 2186)\n",
      "Using p=4778603, hash_functions=874, vocab_size=2186\n",
      "Signature matrix shape: (874, 576)\n",
      "\n",
      "--- MinHash Signature Computation ---\n",
      "  Test set signature matrix shape: (874, 576)\n",
      "\n",
      "--- Ground Truth (Test Set) ---\n",
      "  Duplicate clusters: 50\n",
      "  Total true duplicate pairs: 54\n",
      "\n",
      "--- Testing Multiple Thresholds on Test Set ---\n",
      "  Testing 9 thresholds: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
      "  Completed testing 9 thresholds\n",
      "\n",
      "--- Evaluation with Best Threshold (for reference) ---\n",
      "  Threshold used: 0.3 (selected from training set)\n",
      "  LSH parameters: b=437, r=2 (optimized on training set)\n",
      "  Candidate pairs: 101173\n",
      "  True positives (TP): 54\n",
      "  Pair Quality (PQ): 0.000534 (0.0534%)\n",
      "  Pair Completeness (PC): 1.000000 (100.0000%)\n",
      "  F1 Score: 0.001067 (0.1067%)\n",
      "\n",
      "--- Filtering Test Set Candidate Pairs ---\n",
      "Original candidate pairs: 101173\n",
      "Removed (different brands): 69260\n",
      "Removed (same shop): 13417\n",
      "Filtered pairs remaining: 18496\n",
      "Binary vectors shape: (1048, 2186)\n",
      "Using p=4778603, hash_functions=874, vocab_size=2186\n",
      "Signature matrix shape: (874, 1048)\n",
      "\n",
      "--- Filtering Training Set Candidate Pairs ---\n",
      "Original candidate pairs: 354067\n",
      "Removed (different brands): 254032\n",
      "Removed (same shop): 40187\n",
      "Filtered pairs remaining: 59848\n",
      "\n",
      "--- Optimizing Clustering Distance Threshold on Training Set ---\n",
      "\n",
      "======================================================================\n",
      "FINDING BEST CLUSTERING DISTANCE THRESHOLD ON TRAINING SET\n",
      "======================================================================\n",
      "F1 score upper bound constraint: F1 <= 0.20\n",
      "Testing distance thresholds: [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
      "Linkage: complete\n",
      "Training set size: 1048 products\n",
      "Filtered pairs: 59848\n",
      "Total true duplicate pairs in training set: 169\n",
      "Duplicate clusters: 153\n",
      "\n",
      "Testing thresholds...\n",
      "Threshold    Clusters   TP     PQ         PC         F1         Status    \n",
      "----------------------------------------------------------------------\n",
      "  Computed TMWM similarity for 59848 pairs\n",
      "0.20         969        4      0.047619   0.023669   0.031621   ✓ IN RANGE\n",
      "  Computed TMWM similarity for 59848 pairs\n",
      "0.30         899        8      0.049689   0.047337   0.048485   ✓ IN RANGE\n",
      "  Computed TMWM similarity for 59848 pairs\n",
      "0.40         852        8      0.037915   0.047337   0.042105   ✓ IN RANGE\n",
      "  Computed TMWM similarity for 59848 pairs\n",
      "0.50         806        11     0.042308   0.065089   0.051282   ✓ IN RANGE\n",
      "  Computed TMWM similarity for 59848 pairs\n",
      "0.60         694        14     0.036176   0.082840   0.050360   ✓ IN RANGE\n",
      "  Computed TMWM similarity for 59848 pairs\n",
      "0.70         636        14     0.030702   0.082840   0.044800   ✓ IN RANGE\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Best threshold: 0.5 (F1 = 0.051282) [F1 <= 0.20 ✓]\n",
      "======================================================================\n",
      "\n",
      "--- Best Clustering Distance Threshold Selected ---\n",
      "  Distance threshold: 0.5\n",
      "  F1 Score on Training: 0.051282\n",
      "  (This means similarity >= 0.50 for clustering)\n",
      "\n",
      "--- Agglomerative Clustering on Test Set (with Optimized Threshold) ---\n",
      "  Using optimized distance threshold: 0.5 (F1 on training: 0.051282)\n",
      "  This means similarity >= 0.50 for clustering\n",
      "  Computed TMWM similarity for 18496 pairs\n",
      "\n",
      "  Clustering Results (Test Set):\n",
      "    Number of clusters: 561\n",
      "    Clusters with 2+ products: 15\n",
      "    True positives (TP): 8\n",
      "    Total predicted pairs: 15\n",
      "    Total true pairs: 54\n",
      "    Precision (PQ): 0.533333 (53.3333%)\n",
      "    Recall (PC): 0.148148 (14.8148%)\n",
      "    F1 Score: 0.231884 (23.1884%)\n",
      "\n",
      "  Overfitting check: Gap = -0.180602 (-18.06%) - OK\n",
      "\n",
      "======================================================================\n",
      "BOOTSTRAP ITERATION 2/5\n",
      "======================================================================\n",
      "\n",
      "--- Bootstrap 2 Split ---\n",
      "  Training set: 1624 samples (1011 unique) -> using 1011 unique products\n",
      "  Test set: 613 samples (613 unique) -> using 613 unique products\n",
      "\n",
      "======================================================================\n",
      "METHOD: CURRENT\n",
      "======================================================================\n",
      "\n",
      "--- Model Word Extraction (current) ---\n",
      "  Extracted model words for 1011 training products\n",
      "  Extracted model words for 613 test products\n",
      "\n",
      "--- Model Word Extraction ---\n",
      "  Extracted model words for 1011 training products\n",
      "  Extracted model words for 613 test products\n",
      "\n",
      "--- Vocabulary Building ---\n",
      "  Training set unique model words: 1647\n",
      "  Test set unique model words: 1158\n",
      "  Combined vocabulary size: 2186\n",
      "\n",
      "======================================================================\n",
      "FINDING BEST THRESHOLD ON TRAINING SET\n",
      "======================================================================\n",
      "F1 score upper bound constraint: F1 <= 0.20\n",
      "Testing thresholds: [0.3, 0.2, 0.4]\n",
      "Training set size: 1011 products\n",
      "Total true duplicate pairs in training set: 172\n",
      "Duplicate clusters: 143\n",
      "\n",
      "Testing thresholds...\n",
      "Threshold    Candidates   TP     PQ         PC         F1        \n",
      "----------------------------------------------------------------------\n",
      "Binary vectors shape: (1011, 2186)\n",
      "Using p=4778603, hash_functions=874, vocab_size=2186\n",
      "Signature matrix shape: (874, 1011)\n",
      "Signature matrix shape: (874, 1011)\n",
      "Number of hash functions: 874\n",
      "\n",
      "Testing threshold 0.3...\n",
      "    Theoretical (b, r) = (437, 2) for threshold 0.3\n",
      "    Achieved threshold: 0.047836 (error: 0.252164)\n",
      "0.30         294801       172    0.000583   1.000000   0.001166   (b=437, r=2) ✓\n",
      "\n",
      "Testing threshold 0.2...\n",
      "    Theoretical (b, r) = (437, 2) for threshold 0.2\n",
      "    Achieved threshold: 0.047836 (error: 0.152164)\n",
      "0.20         294801       172    0.000583   1.000000   0.001166   (b=437, r=2) ✓\n",
      "\n",
      "Testing threshold 0.4...\n",
      "    Theoretical (b, r) = (437, 2) for threshold 0.4\n",
      "    Achieved threshold: 0.047836 (error: 0.352164)\n",
      "0.40         294801       172    0.000583   1.000000   0.001166   (b=437, r=2) ✓\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Best threshold: 0.3 (F1 = 0.001166) [F1 <= 0.20 ✓]\n",
      "======================================================================\n",
      "\n",
      "--- Best Threshold Selection ---\n",
      "  Selected threshold: 0.3 (F1 on training: 0.001166)\n",
      "  Optimal LSH parameters: b=437, r=2\n",
      "Binary vectors shape: (613, 2186)\n",
      "\n",
      "--- Binary Vector Creation ---\n",
      "  Test set binary vectors shape: (613, 2186)\n",
      "Using p=4778603, hash_functions=874, vocab_size=2186\n",
      "Signature matrix shape: (874, 613)\n",
      "\n",
      "--- MinHash Signature Computation ---\n",
      "  Test set signature matrix shape: (874, 613)\n",
      "\n",
      "--- Ground Truth (Test Set) ---\n",
      "  Duplicate clusters: 51\n",
      "  Total true duplicate pairs: 55\n",
      "\n",
      "--- Testing Multiple Thresholds on Test Set ---\n",
      "  Testing 9 thresholds: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
      "  Completed testing 9 thresholds\n",
      "\n",
      "--- Evaluation with Best Threshold (for reference) ---\n",
      "  Threshold used: 0.3 (selected from training set)\n",
      "  LSH parameters: b=437, r=2 (optimized on training set)\n",
      "  Candidate pairs: 94722\n",
      "  True positives (TP): 55\n",
      "  Pair Quality (PQ): 0.000581 (0.0581%)\n",
      "  Pair Completeness (PC): 1.000000 (100.0000%)\n",
      "  F1 Score: 0.001161 (0.1161%)\n",
      "\n",
      "--- Filtering Test Set Candidate Pairs ---\n",
      "Original candidate pairs: 94722\n",
      "Removed (different brands): 62157\n",
      "Removed (same shop): 14006\n",
      "Filtered pairs remaining: 18559\n",
      "Binary vectors shape: (1011, 2186)\n",
      "Using p=4778603, hash_functions=874, vocab_size=2186\n",
      "Signature matrix shape: (874, 1011)\n",
      "\n",
      "--- Filtering Training Set Candidate Pairs ---\n",
      "Original candidate pairs: 327940\n",
      "Removed (different brands): 233753\n",
      "Removed (same shop): 37867\n",
      "Filtered pairs remaining: 56320\n",
      "\n",
      "--- Optimizing Clustering Distance Threshold on Training Set ---\n",
      "\n",
      "======================================================================\n",
      "FINDING BEST CLUSTERING DISTANCE THRESHOLD ON TRAINING SET\n",
      "======================================================================\n",
      "F1 score upper bound constraint: F1 <= 0.20\n",
      "Testing distance thresholds: [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
      "Linkage: complete\n",
      "Training set size: 1011 products\n",
      "Filtered pairs: 56320\n",
      "Total true duplicate pairs in training set: 172\n",
      "Duplicate clusters: 143\n",
      "\n",
      "Testing thresholds...\n",
      "Threshold    Clusters   TP     PQ         PC         F1         Status    \n",
      "----------------------------------------------------------------------\n",
      "  Computed TMWM similarity for 56320 pairs\n",
      "0.20         943        1      0.013699   0.005814   0.008163   ✓ IN RANGE\n",
      "  Computed TMWM similarity for 56320 pairs\n",
      "0.30         860        2      0.012422   0.011628   0.012012   ✓ IN RANGE\n",
      "  Computed TMWM similarity for 56320 pairs\n",
      "0.40         822        2      0.009901   0.011628   0.010695   ✓ IN RANGE\n",
      "  Computed TMWM similarity for 56320 pairs\n",
      "0.50         779        4      0.016064   0.023256   0.019002   ✓ IN RANGE\n",
      "  Computed TMWM similarity for 56320 pairs\n",
      "0.60         667        7      0.018182   0.040698   0.025135   ✓ IN RANGE\n",
      "  Computed TMWM similarity for 56320 pairs\n",
      "0.70         611        6      0.013043   0.034884   0.018987   ✓ IN RANGE\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Best threshold: 0.6 (F1 = 0.025135) [F1 <= 0.20 ✓]\n",
      "======================================================================\n",
      "\n",
      "--- Best Clustering Distance Threshold Selected ---\n",
      "  Distance threshold: 0.6\n",
      "  F1 Score on Training: 0.025135\n",
      "  (This means similarity >= 0.40 for clustering)\n",
      "\n",
      "--- Agglomerative Clustering on Test Set (with Optimized Threshold) ---\n",
      "  Using optimized distance threshold: 0.6 (F1 on training: 0.025135)\n",
      "  This means similarity >= 0.40 for clustering\n",
      "  Computed TMWM similarity for 18559 pairs\n",
      "\n",
      "  Clustering Results (Test Set):\n",
      "    Number of clusters: 525\n",
      "    Clusters with 2+ products: 88\n",
      "    True positives (TP): 20\n",
      "    Total predicted pairs: 88\n",
      "    Total true pairs: 55\n",
      "    Precision (PQ): 0.227273 (22.7273%)\n",
      "    Recall (PC): 0.363636 (36.3636%)\n",
      "    F1 Score: 0.279720 (27.9720%)\n",
      "\n",
      "  Overfitting check: Gap = -0.254586 (-25.46%) - OK\n",
      "\n",
      "======================================================================\n",
      "BOOTSTRAP ITERATION 3/5\n",
      "======================================================================\n",
      "\n",
      "--- Bootstrap 3 Split ---\n",
      "  Training set: 1624 samples (1040 unique) -> using 1040 unique products\n",
      "  Test set: 584 samples (584 unique) -> using 584 unique products\n",
      "\n",
      "======================================================================\n",
      "METHOD: CURRENT\n",
      "======================================================================\n",
      "\n",
      "--- Model Word Extraction (current) ---\n",
      "  Extracted model words for 1040 training products\n",
      "  Extracted model words for 584 test products\n",
      "\n",
      "--- Model Word Extraction ---\n",
      "  Extracted model words for 1040 training products\n",
      "  Extracted model words for 584 test products\n",
      "\n",
      "--- Vocabulary Building ---\n",
      "  Training set unique model words: 1666\n",
      "  Test set unique model words: 1137\n",
      "  Combined vocabulary size: 2186\n",
      "\n",
      "======================================================================\n",
      "FINDING BEST THRESHOLD ON TRAINING SET\n",
      "======================================================================\n",
      "F1 score upper bound constraint: F1 <= 0.20\n",
      "Testing thresholds: [0.3, 0.2, 0.4]\n",
      "Training set size: 1040 products\n",
      "Total true duplicate pairs in training set: 160\n",
      "Duplicate clusters: 145\n",
      "\n",
      "Testing thresholds...\n",
      "Threshold    Candidates   TP     PQ         PC         F1        \n",
      "----------------------------------------------------------------------\n",
      "Binary vectors shape: (1040, 2186)\n",
      "Using p=4778603, hash_functions=874, vocab_size=2186\n",
      "Signature matrix shape: (874, 1040)\n",
      "Signature matrix shape: (874, 1040)\n",
      "Number of hash functions: 874\n",
      "\n",
      "Testing threshold 0.3...\n",
      "    Theoretical (b, r) = (437, 2) for threshold 0.3\n",
      "    Achieved threshold: 0.047836 (error: 0.252164)\n",
      "0.30         322345       160    0.000496   1.000000   0.000992   (b=437, r=2) ✓\n",
      "\n",
      "Testing threshold 0.2...\n",
      "    Theoretical (b, r) = (437, 2) for threshold 0.2\n",
      "    Achieved threshold: 0.047836 (error: 0.152164)\n",
      "0.20         322345       160    0.000496   1.000000   0.000992   (b=437, r=2) ✓\n",
      "\n",
      "Testing threshold 0.4...\n",
      "    Theoretical (b, r) = (437, 2) for threshold 0.4\n",
      "    Achieved threshold: 0.047836 (error: 0.352164)\n",
      "0.40         322345       160    0.000496   1.000000   0.000992   (b=437, r=2) ✓\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Best threshold: 0.3 (F1 = 0.000992) [F1 <= 0.20 ✓]\n",
      "======================================================================\n",
      "\n",
      "--- Best Threshold Selection ---\n",
      "  Selected threshold: 0.3 (F1 on training: 0.000992)\n",
      "  Optimal LSH parameters: b=437, r=2\n",
      "Binary vectors shape: (584, 2186)\n",
      "\n",
      "--- Binary Vector Creation ---\n",
      "  Test set binary vectors shape: (584, 2186)\n",
      "Using p=4778603, hash_functions=874, vocab_size=2186\n",
      "Signature matrix shape: (874, 584)\n",
      "\n",
      "--- MinHash Signature Computation ---\n",
      "  Test set signature matrix shape: (874, 584)\n",
      "\n",
      "--- Ground Truth (Test Set) ---\n",
      "  Duplicate clusters: 58\n",
      "  Total true duplicate pairs: 64\n",
      "\n",
      "--- Testing Multiple Thresholds on Test Set ---\n",
      "  Testing 9 thresholds: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
      "  Completed testing 9 thresholds\n",
      "\n",
      "--- Evaluation with Best Threshold (for reference) ---\n",
      "  Threshold used: 0.3 (selected from training set)\n",
      "  LSH parameters: b=437, r=2 (optimized on training set)\n",
      "  Candidate pairs: 110003\n",
      "  True positives (TP): 63\n",
      "  Pair Quality (PQ): 0.000573 (0.0573%)\n",
      "  Pair Completeness (PC): 0.984375 (98.4375%)\n",
      "  F1 Score: 0.001145 (0.1145%)\n",
      "\n",
      "--- Filtering Test Set Candidate Pairs ---\n",
      "Original candidate pairs: 110003\n",
      "Removed (different brands): 78345\n",
      "Removed (same shop): 12944\n",
      "Filtered pairs remaining: 18714\n",
      "Binary vectors shape: (1040, 2186)\n",
      "Using p=4778603, hash_functions=874, vocab_size=2186\n",
      "Signature matrix shape: (874, 1040)\n",
      "\n",
      "--- Filtering Training Set Candidate Pairs ---\n",
      "Original candidate pairs: 259237\n",
      "Removed (different brands): 170191\n",
      "Removed (same shop): 36516\n",
      "Filtered pairs remaining: 52530\n",
      "\n",
      "--- Optimizing Clustering Distance Threshold on Training Set ---\n",
      "\n",
      "======================================================================\n",
      "FINDING BEST CLUSTERING DISTANCE THRESHOLD ON TRAINING SET\n",
      "======================================================================\n",
      "F1 score upper bound constraint: F1 <= 0.20\n",
      "Testing distance thresholds: [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
      "Linkage: complete\n",
      "Training set size: 1040 products\n",
      "Filtered pairs: 52530\n",
      "Total true duplicate pairs in training set: 160\n",
      "Duplicate clusters: 145\n",
      "\n",
      "Testing thresholds...\n",
      "Threshold    Clusters   TP     PQ         PC         F1         Status    \n",
      "----------------------------------------------------------------------\n",
      "  Computed TMWM similarity for 52530 pairs\n",
      "0.20         967        2      0.025641   0.012500   0.016807   ✓ IN RANGE\n",
      "  Computed TMWM similarity for 52530 pairs\n",
      "0.30         894        4      0.025806   0.025000   0.025397   ✓ IN RANGE\n",
      "  Computed TMWM similarity for 52530 pairs\n",
      "0.40         849        5      0.024272   0.031250   0.027322   ✓ IN RANGE\n",
      "  Computed TMWM similarity for 52530 pairs\n",
      "0.50         795        7      0.026119   0.043750   0.032710   ✓ IN RANGE\n",
      "  Computed TMWM similarity for 52530 pairs\n",
      "0.60         688        13     0.033079   0.081250   0.047016   ✓ IN RANGE\n",
      "  Computed TMWM similarity for 52530 pairs\n",
      "0.70         645        16     0.036199   0.100000   0.053156   ✓ IN RANGE\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Best threshold: 0.7 (F1 = 0.053156) [F1 <= 0.20 ✓]\n",
      "======================================================================\n",
      "\n",
      "--- Best Clustering Distance Threshold Selected ---\n",
      "  Distance threshold: 0.7\n",
      "  F1 Score on Training: 0.053156\n",
      "  (This means similarity >= 0.30 for clustering)\n",
      "\n",
      "--- Agglomerative Clustering on Test Set (with Optimized Threshold) ---\n",
      "  Using optimized distance threshold: 0.7 (F1 on training: 0.053156)\n",
      "  This means similarity >= 0.30 for clustering\n",
      "  Computed TMWM similarity for 18714 pairs\n",
      "\n",
      "  Clustering Results (Test Set):\n",
      "    Number of clusters: 454\n",
      "    Clusters with 2+ products: 127\n",
      "    True positives (TP): 25\n",
      "    Total predicted pairs: 133\n",
      "    Total true pairs: 64\n",
      "    Precision (PQ): 0.187970 (18.7970%)\n",
      "    Recall (PC): 0.390625 (39.0625%)\n",
      "    F1 Score: 0.253807 (25.3807%)\n",
      "\n",
      "  Overfitting check: Gap = -0.200651 (-20.07%) - OK\n",
      "\n",
      "======================================================================\n",
      "BOOTSTRAP ITERATION 4/5\n",
      "======================================================================\n",
      "\n",
      "--- Bootstrap 4 Split ---\n",
      "  Training set: 1624 samples (1024 unique) -> using 1024 unique products\n",
      "  Test set: 600 samples (600 unique) -> using 600 unique products\n",
      "\n",
      "======================================================================\n",
      "METHOD: CURRENT\n",
      "======================================================================\n",
      "\n",
      "--- Model Word Extraction (current) ---\n",
      "  Extracted model words for 1024 training products\n",
      "  Extracted model words for 600 test products\n",
      "\n",
      "--- Model Word Extraction ---\n",
      "  Extracted model words for 1024 training products\n",
      "  Extracted model words for 600 test products\n",
      "\n",
      "--- Vocabulary Building ---\n",
      "  Training set unique model words: 1617\n",
      "  Test set unique model words: 1182\n",
      "  Combined vocabulary size: 2186\n",
      "\n",
      "======================================================================\n",
      "FINDING BEST THRESHOLD ON TRAINING SET\n",
      "======================================================================\n",
      "F1 score upper bound constraint: F1 <= 0.20\n",
      "Testing thresholds: [0.3, 0.2, 0.4]\n",
      "Training set size: 1024 products\n",
      "Total true duplicate pairs in training set: 166\n",
      "Duplicate clusters: 146\n",
      "\n",
      "Testing thresholds...\n",
      "Threshold    Candidates   TP     PQ         PC         F1        \n",
      "----------------------------------------------------------------------\n",
      "Binary vectors shape: (1024, 2186)\n",
      "Using p=4778603, hash_functions=874, vocab_size=2186\n",
      "Signature matrix shape: (874, 1024)\n",
      "Signature matrix shape: (874, 1024)\n",
      "Number of hash functions: 874\n",
      "\n",
      "Testing threshold 0.3...\n",
      "    Theoretical (b, r) = (437, 2) for threshold 0.3\n",
      "    Achieved threshold: 0.047836 (error: 0.252164)\n",
      "0.30         289673       166    0.000573   1.000000   0.001145   (b=437, r=2) ✓\n",
      "\n",
      "Testing threshold 0.2...\n",
      "    Theoretical (b, r) = (437, 2) for threshold 0.2\n",
      "    Achieved threshold: 0.047836 (error: 0.152164)\n",
      "0.20         289673       166    0.000573   1.000000   0.001145   (b=437, r=2) ✓\n",
      "\n",
      "Testing threshold 0.4...\n",
      "    Theoretical (b, r) = (437, 2) for threshold 0.4\n",
      "    Achieved threshold: 0.047836 (error: 0.352164)\n",
      "0.40         289673       166    0.000573   1.000000   0.001145   (b=437, r=2) ✓\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Best threshold: 0.3 (F1 = 0.001145) [F1 <= 0.20 ✓]\n",
      "======================================================================\n",
      "\n",
      "--- Best Threshold Selection ---\n",
      "  Selected threshold: 0.3 (F1 on training: 0.001145)\n",
      "  Optimal LSH parameters: b=437, r=2\n",
      "Binary vectors shape: (600, 2186)\n",
      "\n",
      "--- Binary Vector Creation ---\n",
      "  Test set binary vectors shape: (600, 2186)\n",
      "Using p=4778603, hash_functions=874, vocab_size=2186\n",
      "Signature matrix shape: (874, 600)\n",
      "\n",
      "--- MinHash Signature Computation ---\n",
      "  Test set signature matrix shape: (874, 600)\n",
      "\n",
      "--- Ground Truth (Test Set) ---\n",
      "  Duplicate clusters: 50\n",
      "  Total true duplicate pairs: 50\n",
      "\n",
      "--- Testing Multiple Thresholds on Test Set ---\n",
      "  Testing 9 thresholds: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
      "  Completed testing 9 thresholds\n",
      "\n",
      "--- Evaluation with Best Threshold (for reference) ---\n",
      "  Threshold used: 0.3 (selected from training set)\n",
      "  LSH parameters: b=437, r=2 (optimized on training set)\n",
      "  Candidate pairs: 113480\n",
      "  True positives (TP): 49\n",
      "  Pair Quality (PQ): 0.000432 (0.0432%)\n",
      "  Pair Completeness (PC): 0.980000 (98.0000%)\n",
      "  F1 Score: 0.000863 (0.0863%)\n",
      "\n",
      "--- Filtering Test Set Candidate Pairs ---\n",
      "Original candidate pairs: 113480\n",
      "Removed (different brands): 82829\n",
      "Removed (same shop): 13315\n",
      "Filtered pairs remaining: 17336\n",
      "Binary vectors shape: (1024, 2186)\n",
      "Using p=4778603, hash_functions=874, vocab_size=2186\n",
      "Signature matrix shape: (874, 1024)\n",
      "\n",
      "--- Filtering Training Set Candidate Pairs ---\n",
      "Original candidate pairs: 337904\n",
      "Removed (different brands): 235023\n",
      "Removed (same shop): 41929\n",
      "Filtered pairs remaining: 60952\n",
      "\n",
      "--- Optimizing Clustering Distance Threshold on Training Set ---\n",
      "\n",
      "======================================================================\n",
      "FINDING BEST CLUSTERING DISTANCE THRESHOLD ON TRAINING SET\n",
      "======================================================================\n",
      "F1 score upper bound constraint: F1 <= 0.20\n",
      "Testing distance thresholds: [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
      "Linkage: complete\n",
      "Training set size: 1024 products\n",
      "Filtered pairs: 60952\n",
      "Total true duplicate pairs in training set: 166\n",
      "Duplicate clusters: 146\n",
      "\n",
      "Testing thresholds...\n",
      "Threshold    Clusters   TP     PQ         PC         F1         Status    \n",
      "----------------------------------------------------------------------\n",
      "  Computed TMWM similarity for 60952 pairs\n",
      "0.20         946        5      0.058824   0.030120   0.039841   ✓ IN RANGE\n",
      "  Computed TMWM similarity for 60952 pairs\n",
      "0.30         874        5      0.030488   0.030120   0.030303   ✓ IN RANGE\n",
      "  Computed TMWM similarity for 60952 pairs\n",
      "0.40         826        5      0.023256   0.030120   0.026247   ✓ IN RANGE\n",
      "  Computed TMWM similarity for 60952 pairs\n",
      "0.50         772        8      0.028881   0.048193   0.036117   ✓ IN RANGE\n",
      "  Computed TMWM similarity for 60952 pairs\n",
      "0.60         667        11     0.027228   0.066265   0.038596   ✓ IN RANGE\n",
      "  Computed TMWM similarity for 60952 pairs\n",
      "0.70         612        11     0.023305   0.066265   0.034483   ✓ IN RANGE\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Best threshold: 0.2 (F1 = 0.039841) [F1 <= 0.20 ✓]\n",
      "======================================================================\n",
      "\n",
      "--- Best Clustering Distance Threshold Selected ---\n",
      "  Distance threshold: 0.2\n",
      "  F1 Score on Training: 0.039841\n",
      "  (This means similarity >= 0.80 for clustering)\n",
      "\n",
      "--- Agglomerative Clustering on Test Set (with Optimized Threshold) ---\n",
      "  Using optimized distance threshold: 0.2 (F1 on training: 0.039841)\n",
      "  This means similarity >= 0.80 for clustering\n",
      "  Computed TMWM similarity for 17336 pairs\n",
      "\n",
      "  Clustering Results (Test Set):\n",
      "    Number of clusters: 599\n",
      "    Clusters with 2+ products: 1\n",
      "    True positives (TP): 1\n",
      "    Total predicted pairs: 1\n",
      "    Total true pairs: 50\n",
      "    Precision (PQ): 1.000000 (100.0000%)\n",
      "    Recall (PC): 0.020000 (2.0000%)\n",
      "    F1 Score: 0.039216 (3.9216%)\n",
      "\n",
      "  Overfitting check: Gap = 0.000625 (0.06%) - OK\n",
      "\n",
      "======================================================================\n",
      "BOOTSTRAP ITERATION 5/5\n",
      "======================================================================\n",
      "\n",
      "--- Bootstrap 5 Split ---\n",
      "  Training set: 1624 samples (1009 unique) -> using 1009 unique products\n",
      "  Test set: 615 samples (615 unique) -> using 615 unique products\n",
      "\n",
      "======================================================================\n",
      "METHOD: CURRENT\n",
      "======================================================================\n",
      "\n",
      "--- Model Word Extraction (current) ---\n",
      "  Extracted model words for 1009 training products\n",
      "  Extracted model words for 615 test products\n",
      "\n",
      "--- Model Word Extraction ---\n",
      "  Extracted model words for 1009 training products\n",
      "  Extracted model words for 615 test products\n",
      "\n",
      "--- Vocabulary Building ---\n",
      "  Training set unique model words: 1630\n",
      "  Test set unique model words: 1204\n",
      "  Combined vocabulary size: 2186\n",
      "\n",
      "======================================================================\n",
      "FINDING BEST THRESHOLD ON TRAINING SET\n",
      "======================================================================\n",
      "F1 score upper bound constraint: F1 <= 0.20\n",
      "Testing thresholds: [0.3, 0.2, 0.4]\n",
      "Training set size: 1009 products\n",
      "Total true duplicate pairs in training set: 146\n",
      "Duplicate clusters: 126\n",
      "\n",
      "Testing thresholds...\n",
      "Threshold    Candidates   TP     PQ         PC         F1        \n",
      "----------------------------------------------------------------------\n",
      "Binary vectors shape: (1009, 2186)\n",
      "Using p=4778603, hash_functions=874, vocab_size=2186\n",
      "Signature matrix shape: (874, 1009)\n",
      "Signature matrix shape: (874, 1009)\n",
      "Number of hash functions: 874\n",
      "\n",
      "Testing threshold 0.3...\n",
      "    Theoretical (b, r) = (437, 2) for threshold 0.3\n",
      "    Achieved threshold: 0.047836 (error: 0.252164)\n",
      "0.30         326606       146    0.000447   1.000000   0.000894   (b=437, r=2) ✓\n",
      "\n",
      "Testing threshold 0.2...\n",
      "    Theoretical (b, r) = (437, 2) for threshold 0.2\n",
      "    Achieved threshold: 0.047836 (error: 0.152164)\n",
      "0.20         326606       146    0.000447   1.000000   0.000894   (b=437, r=2) ✓\n",
      "\n",
      "Testing threshold 0.4...\n",
      "    Theoretical (b, r) = (437, 2) for threshold 0.4\n",
      "    Achieved threshold: 0.047836 (error: 0.352164)\n",
      "0.40         326606       146    0.000447   1.000000   0.000894   (b=437, r=2) ✓\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Best threshold: 0.3 (F1 = 0.000894) [F1 <= 0.20 ✓]\n",
      "======================================================================\n",
      "\n",
      "--- Best Threshold Selection ---\n",
      "  Selected threshold: 0.3 (F1 on training: 0.000894)\n",
      "  Optimal LSH parameters: b=437, r=2\n",
      "Binary vectors shape: (615, 2186)\n",
      "\n",
      "--- Binary Vector Creation ---\n",
      "  Test set binary vectors shape: (615, 2186)\n",
      "Using p=4778603, hash_functions=874, vocab_size=2186\n",
      "Signature matrix shape: (874, 615)\n",
      "\n",
      "--- MinHash Signature Computation ---\n",
      "  Test set signature matrix shape: (874, 615)\n",
      "\n",
      "--- Ground Truth (Test Set) ---\n",
      "  Duplicate clusters: 47\n",
      "  Total true duplicate pairs: 51\n",
      "\n",
      "--- Testing Multiple Thresholds on Test Set ---\n",
      "  Testing 9 thresholds: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
      "  Completed testing 9 thresholds\n",
      "\n",
      "--- Evaluation with Best Threshold (for reference) ---\n",
      "  Threshold used: 0.3 (selected from training set)\n",
      "  LSH parameters: b=437, r=2 (optimized on training set)\n",
      "  Candidate pairs: 124059\n",
      "  True positives (TP): 51\n",
      "  Pair Quality (PQ): 0.000411 (0.0411%)\n",
      "  Pair Completeness (PC): 1.000000 (100.0000%)\n",
      "  F1 Score: 0.000822 (0.0822%)\n",
      "\n",
      "--- Filtering Test Set Candidate Pairs ---\n",
      "Original candidate pairs: 124059\n",
      "Removed (different brands): 86682\n",
      "Removed (same shop): 16037\n",
      "Filtered pairs remaining: 21340\n",
      "Binary vectors shape: (1009, 2186)\n",
      "Using p=4778603, hash_functions=874, vocab_size=2186\n",
      "Signature matrix shape: (874, 1009)\n",
      "\n",
      "--- Filtering Training Set Candidate Pairs ---\n",
      "Original candidate pairs: 272925\n",
      "Removed (different brands): 186752\n",
      "Removed (same shop): 33872\n",
      "Filtered pairs remaining: 52301\n",
      "\n",
      "--- Optimizing Clustering Distance Threshold on Training Set ---\n",
      "\n",
      "======================================================================\n",
      "FINDING BEST CLUSTERING DISTANCE THRESHOLD ON TRAINING SET\n",
      "======================================================================\n",
      "F1 score upper bound constraint: F1 <= 0.20\n",
      "Testing distance thresholds: [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
      "Linkage: complete\n",
      "Training set size: 1009 products\n",
      "Filtered pairs: 52301\n",
      "Total true duplicate pairs in training set: 146\n",
      "Duplicate clusters: 126\n",
      "\n",
      "Testing thresholds...\n",
      "Threshold    Clusters   TP     PQ         PC         F1         Status    \n",
      "----------------------------------------------------------------------\n",
      "  Computed TMWM similarity for 52301 pairs\n",
      "0.20         937        1      0.012821   0.006849   0.008929   ✓ IN RANGE\n",
      "  Computed TMWM similarity for 52301 pairs\n",
      "0.30         857        3      0.018182   0.020548   0.019293   ✓ IN RANGE\n",
      "  Computed TMWM similarity for 52301 pairs\n",
      "0.40         811        4      0.018100   0.027397   0.021798   ✓ IN RANGE\n",
      "  Computed TMWM similarity for 52301 pairs\n",
      "0.50         768        4      0.014981   0.027397   0.019370   ✓ IN RANGE\n",
      "  Computed TMWM similarity for 52301 pairs\n",
      "0.60         656        11     0.027228   0.075342   0.040000   ✓ IN RANGE\n",
      "  Computed TMWM similarity for 52301 pairs\n",
      "0.70         603        12     0.025586   0.082192   0.039024   ✓ IN RANGE\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Best threshold: 0.6 (F1 = 0.040000) [F1 <= 0.20 ✓]\n",
      "======================================================================\n",
      "\n",
      "--- Best Clustering Distance Threshold Selected ---\n",
      "  Distance threshold: 0.6\n",
      "  F1 Score on Training: 0.040000\n",
      "  (This means similarity >= 0.40 for clustering)\n",
      "\n",
      "--- Agglomerative Clustering on Test Set (with Optimized Threshold) ---\n",
      "  Using optimized distance threshold: 0.6 (F1 on training: 0.040000)\n",
      "  This means similarity >= 0.40 for clustering\n",
      "  Computed TMWM similarity for 21340 pairs\n",
      "\n",
      "  Clustering Results (Test Set):\n",
      "    Number of clusters: 539\n",
      "    Clusters with 2+ products: 76\n",
      "    True positives (TP): 18\n",
      "    Total predicted pairs: 76\n",
      "    Total true pairs: 51\n",
      "    Precision (PQ): 0.236842 (23.6842%)\n",
      "    Recall (PC): 0.352941 (35.2941%)\n",
      "    F1 Score: 0.283465 (28.3465%)\n",
      "\n",
      "  Overfitting check: Gap = -0.243465 (-24.35%) - OK\n",
      "\n",
      "======================================================================\n",
      "BOOTSTRAP SUMMARY: Aggregated Results Across 5 Iterations\n",
      "======================================================================\n",
      "\n",
      "F1 Score Statistics:\n",
      "  Mean: 0.033454 (3.3454%)\n",
      "  Std:  0.036519 (3.6519%)\n",
      "  Min:  0.000000 (0.0000%)\n",
      "  Max:  0.100000 (10.0000%)\n",
      "\n",
      "Pair Quality (PQ) Statistics:\n",
      "  Mean: 0.161905 (16.1905%)\n",
      "  Std:  0.149375 (14.9375%)\n",
      "\n",
      "Pair Completeness (PC) Statistics:\n",
      "  Mean: 0.019105 (1.9105%)\n",
      "  Std:  0.021488 (2.1488%)\n",
      "\n",
      "Detailed Results per Iteration:\n",
      "Iter   Threshold  F1           PQ           PC           TP     Candidates  \n",
      "--------------------------------------------------------------------------------\n",
      "1      0.90       0.032787     0.142857     0.018519     1      7           \n",
      "2      0.90       0.034483     0.333333     0.018182     1      3           \n",
      "3      0.90       0.000000     0.000000     0.000000     0      8           \n",
      "4      0.90       0.000000     0.000000     0.000000     0      6           \n",
      "5      0.90       0.100000     0.333333     0.058824     3      9           \n",
      "======================================================================\n",
      "\n",
      "Bootstrap evaluation complete!\n",
      "Final F1 Score: 0.033454 ± 0.036519\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BOOTSTRAP EVALUATION: 5 Iterations with F1 Score Calculation\n",
    "# ============================================================================\n",
    "# This code structure runs bootstrap sampling 5 times and calculates F1 on test set\n",
    "# Placeholders indicate where LSH, binary vectors, MinHashing should be inserted\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Clean the data first\n",
    "# Keep a version with shops for filtering later\n",
    "all_values_dict_clean_keep_shops = clean_all_values_dict_advanced(all_values_dict, remove_shops=False)\n",
    "all_values_dict_clean = clean_all_values_dict_advanced(all_values_dict, remove_shops=True)\n",
    "n_samples = len(all_values_dict_clean)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BOOTSTRAP EVALUATION: 5 Iterations\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total samples: {n_samples}\")\n",
    "print(f\"Number of bootstrap iterations: 5\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Store results for each bootstrap iteration\n",
    "bootstrap_results = []\n",
    "\n",
    "# Run 5 bootstrap iterations\n",
    "for bootstrap_idx in range(5):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"BOOTSTRAP ITERATION {bootstrap_idx + 1}/5\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 1: Create Bootstrap Train/Test Split\n",
    "    # ========================================================================\n",
    "    train_indices, test_indices = bootstrap_train_test_split(n_samples)\n",
    "    \n",
    "    # Get unique indices (remove duplicates from bootstrap sampling)\n",
    "    unique_train_indices = np.unique(train_indices)\n",
    "    unique_test_indices = np.unique(test_indices)\n",
    "    \n",
    "    # Create train and test dictionaries using only unique products\n",
    "    dict_train = {i: all_values_dict_clean[idx] for i, idx in enumerate(unique_train_indices)}\n",
    "    dict_test = {i: all_values_dict_clean[idx] for i, idx in enumerate(unique_test_indices)}\n",
    "    \n",
    "    print(f\"\\n--- Bootstrap {bootstrap_idx + 1} Split ---\")\n",
    "    print(f\"  Training set: {len(train_indices)} samples ({len(unique_train_indices)} unique) -> using {len(dict_train)} unique products\")\n",
    "    print(f\"  Test set: {len(test_indices)} samples ({len(unique_test_indices)} unique) -> using {len(dict_test)} unique products\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    \n",
    "    # ========================================================================\n",
    "    # COMPARISON: Run both model word extraction methods\n",
    "    # ========================================================================\n",
    "    methods = [\n",
    "        ('current', extract_all_model_words_paper_method),\n",
    "        ('title_only', extract_model_words_title_only)\n",
    "    ]\n",
    "    \n",
    "    for method_name, extract_func in methods:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"METHOD: {method_name.upper()}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # ========================================================================\n",
    "        # STEP 2: Extract Model Words\n",
    "        # ========================================================================\n",
    "        # Extract model words from train and test sets separately\n",
    "        model_words_train = []\n",
    "        for idx, item in dict_train.items():\n",
    "            mw = extract_func(item)\n",
    "            model_words_train.append(mw)\n",
    "        \n",
    "        model_words_test = []\n",
    "        for idx, item in dict_test.items():\n",
    "            mw = extract_func(item)\n",
    "            model_words_test.append(mw)\n",
    "        \n",
    "        print(f\"\\n--- Model Word Extraction ({method_name}) ---\")\n",
    "        print(f\"  Extracted model words for {len(model_words_train)} training products\")\n",
    "        print(f\"  Extracted model words for {len(model_words_test)} test products\")\n",
    "        \n",
    "        # ========================================================================\n",
    "        # Extract model words from train and test sets separately\n",
    "        model_words_train = []\n",
    "        for idx, item in dict_train.items():\n",
    "           mw = extract_all_model_words_paper_method(item)\n",
    "           model_words_train.append(mw)\n",
    "    \n",
    "        model_words_test = []\n",
    "        for idx, item in dict_test.items():\n",
    "            mw = extract_all_model_words_paper_method(item)\n",
    "            model_words_test.append(mw)\n",
    "    \n",
    "        print(\"\\n--- Model Word Extraction ---\")\n",
    "        print(f\"  Extracted model words for {len(model_words_train)} training products\")\n",
    "        print(f\"  Extracted model words for {len(model_words_test)} test products\")\n",
    "    \n",
    "        # ========================================================================\n",
    "        # STEP 3: Build Vocabulary\n",
    "        # ========================================================================\n",
    "        # Build vocabulary from BOTH training and test sets\n",
    "        model_words_set_train = set()\n",
    "        for mw_list in model_words_train:\n",
    "            model_words_set_train.update(mw_list)\n",
    "    \n",
    "        model_words_set_test = set()\n",
    "        for mw_list in model_words_test:\n",
    "             model_words_set_test.update(mw_list)\n",
    "    \n",
    "        # Combined vocabulary\n",
    "        combined_vocab = sorted(model_words_set_train | model_words_set_test)\n",
    "        word_to_index = {word: idx for idx, word in enumerate(combined_vocab)}\n",
    "        vocab = combined_vocab\n",
    "    \n",
    "        print(\"\\n--- Vocabulary Building ---\")\n",
    "        print(f\"  Training set unique model words: {len(model_words_set_train)}\")\n",
    "        print(f\"  Test set unique model words: {len(model_words_set_test)}\")\n",
    "        print(f\"  Combined vocabulary size: {len(vocab)}\")\n",
    "    \n",
    "        # ========================================================================\n",
    "        # STEP 3.5: Find Best Threshold on Training Set\n",
    "        # ========================================================================\n",
    "        best_threshold, best_f1_train, threshold_results = find_best_threshold_on_training_set(\n",
    "        dict_train, model_words_train, vocab, word_to_index,\n",
    "        thresholds=[0.3,0.2,0.4], f1_range=(0.0, 0.2), verbose=True\n",
    "        )\n",
    "    \n",
    "        # Extract optimal (b, r) for the best threshold\n",
    "        best_b = threshold_results[best_threshold]['best_b']\n",
    "        best_r = threshold_results[best_threshold]['best_r']\n",
    "    \n",
    "        print(f\"\\n--- Best Threshold Selection ---\")\n",
    "        print(f\"  Selected threshold: {best_threshold} (F1 on training: {best_f1_train:.6f})\")\n",
    "        print(f\"  Optimal LSH parameters: b={best_b}, r={best_r}\")\n",
    "    \n",
    "        # ========================================================================\n",
    "        # STEP 4: Create Binary Vectors for Test Set\n",
    "        # ========================================================================\n",
    "        A_test = create_binary_vectors(model_words_test, vocab, word_to_index)\n",
    "    \n",
    "        print(\"\\n--- Binary Vector Creation ---\")\n",
    "        print(f\"  Test set binary vectors shape: {A_test.shape}\")\n",
    "    \n",
    "        # ========================================================================\n",
    "        # STEP 5: Compute MinHash Signatures for Test Set\n",
    "        # ========================================================================\n",
    "        L_test, _ = compute_minhash_signatures(A_test)\n",
    "    \n",
    "        print(\"\\n--- MinHash Signature Computation ---\")\n",
    "        print(f\"  Test set signature matrix shape: {L_test.shape}\")\n",
    "    \n",
    "        # ========================================================================\n",
    "        # STEP 6: Build Ground Truth for Test Set (needed for all thresholds)\n",
    "        # ========================================================================\n",
    "        modelID_to_indices_test = defaultdict(list)\n",
    "        for idx, item in dict_test.items():\n",
    "            if 'modelID' in item:\n",
    "                modelID_to_indices_test[item['modelID']].append(idx)\n",
    "    \n",
    "        # Keep only modelIDs with 2+ products (actual duplicates)\n",
    "        duplicate_clusters_test = {mid: set(idxs) for mid, idxs in modelID_to_indices_test.items() if len(idxs) > 1}\n",
    "    \n",
    "        # Calculate total true duplicate pairs in test set\n",
    "        total_true_pairs_test = sum(len(idxs) * (len(idxs) - 1) // 2 for idxs in duplicate_clusters_test.values())\n",
    "    \n",
    "        print(f\"\\n--- Ground Truth (Test Set) ---\")\n",
    "        print(f\"  Duplicate clusters: {len(duplicate_clusters_test)}\")\n",
    "        print(f\"  Total true duplicate pairs: {total_true_pairs_test}\")\n",
    "    \n",
    "        # ========================================================================\n",
    "        # STEP 7: Test Multiple Thresholds on Test Set for Smooth Curve\n",
    "        # ========================================================================\n",
    "        # Test multiple thresholds to get data points across different fractions\n",
    "        test_thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "        product_ids_test = list(dict_test.keys())\n",
    "    \n",
    "        print(f\"\\n--- Testing Multiple Thresholds on Test Set ---\")\n",
    "        print(f\"  Testing {len(test_thresholds)} thresholds: {test_thresholds}\")\n",
    "    \n",
    "        # Store results for each threshold\n",
    "        for threshold in test_thresholds:\n",
    "        # Apply LSH banding with this threshold\n",
    "             candidate_pairs = lsh_banding(L_test, product_ids_test, t=threshold, verbose=False)\n",
    "        \n",
    "        # Count true duplicates\n",
    "        TP = count_true_duplicates_in_candidates(dict_test, candidate_pairs)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        PQ = TP / len(candidate_pairs) if len(candidate_pairs) > 0 else 0.0\n",
    "        PC = TP / total_true_pairs_test if total_true_pairs_test > 0 else 0.0\n",
    "        F1 = 2 * (PQ * PC) / (PQ + PC) if (PQ + PC) > 0 else 0.0\n",
    "        \n",
    "        # Store results for this threshold\n",
    "        bootstrap_results.append({\n",
    "        'iteration': bootstrap_idx + 1,\n",
    "        'threshold': threshold,\n",
    "        'train_size': len(train_indices),\n",
    "        'train_unique': len(np.unique(train_indices)),\n",
    "        'test_size': len(test_indices),\n",
    "        'candidate_pairs': len(candidate_pairs),\n",
    "        'TP': TP,\n",
    "        'PQ': PQ,\n",
    "        'PC': PC,\n",
    "        'F1': F1,\n",
    "        'total_true_pairs': total_true_pairs_test,\n",
    "            'method': method_name})\n",
    "    \n",
    "        print(f\"  Completed testing {len(test_thresholds)} thresholds\")\n",
    "    \n",
    "        # ========================================================================\n",
    "        # STEP 8: Also evaluate with best threshold from training (for comparison)\n",
    "        # ========================================================================\n",
    "        # Use theoretical (b, r) values calculated from best threshold: t = (1/b)^(1/r)\n",
    "        # These are the same (b, r) used on training set for this threshold\n",
    "        candidate_pairs_best = lsh_banding(L_test, product_ids_test, t=best_threshold, b=best_b, r=best_r, verbose=False)\n",
    "        TP_best = count_true_duplicates_in_candidates(dict_test, candidate_pairs_best)\n",
    "        PQ_best = TP_best / len(candidate_pairs_best) if len(candidate_pairs_best) > 0 else 0.0\n",
    "        PC_best = TP_best / total_true_pairs_test if total_true_pairs_test > 0 else 0.0\n",
    "        F1_best = 2 * (PQ_best * PC_best) / (PQ_best + PC_best) if (PQ_best + PC_best) > 0 else 0.0\n",
    "    \n",
    "        print(f\"\\n--- Evaluation with Best Threshold (for reference) ---\")\n",
    "        print(f\"  Threshold used: {best_threshold} (selected from training set)\")\n",
    "        print(f\"  LSH parameters: b={best_b}, r={best_r} (optimized on training set)\")\n",
    "        print(f\"  Candidate pairs: {len(candidate_pairs_best)}\")\n",
    "        print(f\"  True positives (TP): {TP_best}\")\n",
    "        print(f\"  Pair Quality (PQ): {PQ_best:.6f} ({PQ_best*100:.4f}%)\")\n",
    "        print(f\"  Pair Completeness (PC): {PC_best:.6f} ({PC_best*100:.4f}%)\")\n",
    "        print(f\"  F1 Score: {F1_best:.6f} ({F1_best*100:.4f}%)\")\n",
    "    \n",
    "        # Use best threshold candidate pairs for filtering (if needed)\n",
    "        candidate_pairs_for_eval = candidate_pairs_best\n",
    "    \n",
    "        # ========================================================================\n",
    "        # STEP 10: Apply Filtering to Test Set\n",
    "        # ========================================================================\n",
    "        # Create dict_test_with_shops for shop extraction (shops preserved)\n",
    "        # Map from dict_test keys (0, 1, 2, ...) to original indices in all_values_dict_clean_keep_shops\n",
    "        dict_test_with_shops = {i: all_values_dict_clean_keep_shops[test_indices[i]] for i in dict_test.keys()}\n",
    "    \n",
    "        print(f\"\\n--- Filtering Test Set Candidate Pairs ---\")\n",
    "        filtered_pairs_test, filter_stats_test = filter_candidate_pairs_by_brand_and_shop(\n",
    "        candidate_pairs_for_eval,\n",
    "        dict_test,\n",
    "        dict_data_with_shops=dict_test_with_shops,\n",
    "        verbose=True\n",
    "        )\n",
    "    \n",
    "        # ========================================================================\n",
    "        # STEP 10.5: Clustering will be applied AFTER optimization (see STEP 11.6)\n",
    "        # ========================================================================\n",
    "        # (Moved to after training set optimization to use optimized threshold)\n",
    "        # STEP 11: Apply Filtering to Training Set\n",
    "        # ========================================================================\n",
    "        # First, get candidate pairs from training set using best threshold\n",
    "        A_train = create_binary_vectors(model_words_train, vocab, word_to_index)\n",
    "        L_train, _ = compute_minhash_signatures(A_train)\n",
    "        product_ids_train = list(dict_train.keys())\n",
    "        candidate_pairs_train = lsh_banding(L_train, product_ids_train, t=best_threshold, verbose=False)\n",
    "    \n",
    "        # Create dict_train_with_shops for shop extraction (shops preserved)\n",
    "        # Map from dict_train keys (0, 1, 2, ...) to original indices in all_values_dict_clean_keep_shops\n",
    "        dict_train_with_shops = {i: all_values_dict_clean_keep_shops[train_indices[i]] for i in dict_train.keys()}\n",
    "    \n",
    "        print(f\"\\n--- Filtering Training Set Candidate Pairs ---\")\n",
    "        filtered_pairs_train, filter_stats_train = filter_candidate_pairs_by_brand_and_shop(\n",
    "        candidate_pairs_train,\n",
    "        dict_train,\n",
    "        dict_data_with_shops=dict_train_with_shops,\n",
    "        verbose=True\n",
    "        )\n",
    "    \n",
    "        # ========================================================================\n",
    "        # STEP 11.5: Optimize Clustering Distance Threshold on Training Set\n",
    "        # ========================================================================\n",
    "        print(f\"\\n--- Optimizing Clustering Distance Threshold on Training Set ---\")\n",
    "    \n",
    "        # Get all product IDs from training set\n",
    "        all_product_ids_train = list(dict_train.keys())\n",
    "    \n",
    "        # Find best distance threshold on training set\n",
    "        best_clustering_threshold, best_clustering_f1_train, clustering_threshold_results = find_best_clustering_distance_threshold(\n",
    "        filtered_pairs_train,\n",
    "        all_product_ids_train,\n",
    "        dict_train,\n",
    "        linkage='complete',\n",
    "        distance_thresholds=[0.2, 0.3, 0.4, 0.5, 0.6, 0.7],\n",
    "        f1_range=(0.0, 0.2),  # F1 upper bound constraint (F1 <= 0.2) to prevent overfitting\n",
    "        verbose=True\n",
    "        )\n",
    "    \n",
    "        print(f\"\\n--- Best Clustering Distance Threshold Selected ---\")\n",
    "        print(f\"  Distance threshold: {best_clustering_threshold}\")\n",
    "        print(f\"  F1 Score on Training: {best_clustering_f1_train:.6f}\")\n",
    "        print(f\"  (This means similarity >= {1.0 - best_clustering_threshold:.2f} for clustering)\")\n",
    "    \n",
    "        # ========================================================================\n",
    "        # STEP 11.6: Apply Agglomerative Clustering to Test Set (with Optimized Threshold)\n",
    "        # ========================================================================\n",
    "        print(f\"\\n--- Agglomerative Clustering on Test Set (with Optimized Threshold) ---\")\n",
    "    \n",
    "        # Get all product IDs from test set\n",
    "        all_product_ids_test = list(dict_test.keys())\n",
    "    \n",
    "        # Apply clustering with optimized distance threshold from training set\n",
    "        # NOTE: Uses TMWM similarity computed as overlap coefficient: |model_words_1 ∩ model_words_2| / min(|model_words_1|, |model_words_2|)\n",
    "        print(f\"  Using optimized distance threshold: {best_clustering_threshold} (F1 on training: {best_clustering_f1_train:.6f})\")\n",
    "        print(f\"  This means similarity >= {1.0 - best_clustering_threshold:.2f} for clustering\")\n",
    "    \n",
    "        cluster_labels, clustering_model = apply_agglomerative_clustering(\n",
    "        filtered_pairs_test,\n",
    "        all_product_ids_test,\n",
    "        dict_test,  # Pass dict_data for TMWM similarity computation\n",
    "        linkage='complete',  # Complete linkage = maximum linkage (double linkage)\n",
    "        distance_threshold=best_clustering_threshold  # Use optimized threshold from training set\n",
    "        )\n",
    "    \n",
    "        # Evaluate clustering using F1 score\n",
    "        f1_clustering, precision_clustering, recall_clustering, TP_clustering, \\\n",
    "        total_predicted_pairs, total_true_pairs_clustering = evaluate_clustering_f1_score(\n",
    "        cluster_labels,\n",
    "        dict_test,\n",
    "        all_product_ids_test\n",
    "        )\n",
    "    \n",
    "        print(f\"\\n  Clustering Results (Test Set):\")\n",
    "        print(f\"    Number of clusters: {len(set(cluster_labels))}\")\n",
    "        print(f\"    Clusters with 2+ products: {sum(1 for label in set(cluster_labels) if list(cluster_labels).count(label) > 1)}\")\n",
    "        print(f\"    True positives (TP): {TP_clustering}\")\n",
    "        print(f\"    Total predicted pairs: {total_predicted_pairs}\")\n",
    "        print(f\"    Total true pairs: {total_true_pairs_clustering}\")\n",
    "        print(f\"    Precision (PQ): {precision_clustering:.6f} ({precision_clustering*100:.4f}%)\")\n",
    "        print(f\"    Recall (PC): {recall_clustering:.6f} ({recall_clustering*100:.4f}%)\")\n",
    "        print(f\"    F1 Score: {f1_clustering:.6f} ({f1_clustering*100:.4f}%)\")\n",
    "    \n",
    "        # Check for overfitting: if test F1 is much lower than training F1, warn\n",
    "        f1_gap = best_clustering_f1_train - f1_clustering\n",
    "        if f1_gap > 0.10:  # More than 10% gap suggests overfitting\n",
    "            print(f\"\\n  WARNING: Potential overfitting detected!\")\n",
    "            print(f\"    Training F1: {best_clustering_f1_train:.6f}\")\n",
    "            print(f\"    Test F1: {f1_clustering:.6f}\")\n",
    "            print(f\"    Gap: {f1_gap:.6f} ({f1_gap*100:.2f}%)\")\n",
    "        else:\n",
    "            print(f\"\\n  Overfitting check: Gap = {f1_gap:.6f} ({f1_gap*100:.2f}%) - OK\")\n",
    "    \n",
    "        # Store clustering results for the best threshold evaluation\n",
    "        # Find the result entry that matches the best threshold and current iteration\n",
    "        if len(bootstrap_results) > 0:\n",
    "            current_iteration = bootstrap_idx + 1\n",
    "        for result in reversed(bootstrap_results):\n",
    "            if result.get('threshold') == best_threshold and result.get('iteration') == current_iteration:\n",
    "                result['clustering_f1'] = f1_clustering\n",
    "                result['clustering_precision'] = precision_clustering\n",
    "                result['clustering_recall'] = recall_clustering\n",
    "                result['clustering_TP'] = TP_clustering\n",
    "                result['clustering_n_clusters'] = len(set(cluster_labels))\n",
    "                result['clustering_total_predicted_pairs'] = total_predicted_pairs\n",
    "                result['clustering_total_true_pairs'] = total_true_pairs_clustering\n",
    "                result['clustering_f1_train'] = best_clustering_f1_train\n",
    "                result['clustering_f1_gap'] = f1_gap\n",
    "        break\n",
    "    \n",
    "        # Store filtered results\n",
    "        bootstrap_results[-1]['filtered_pairs_test'] = len(filtered_pairs_test)\n",
    "        bootstrap_results[-1]['filtered_pairs_train'] = len(filtered_pairs_train)\n",
    "        bootstrap_results[-1]['filter_stats_test'] = filter_stats_test\n",
    "        bootstrap_results[-1]['filter_stats_train'] = filter_stats_train\n",
    "        bootstrap_results[-1]['best_clustering_threshold'] = best_clustering_threshold\n",
    "        bootstrap_results[-1]['best_clustering_f1_train'] = best_clustering_f1_train\n",
    "\n",
    "        # ========================================================================\n",
    "        \n",
    "# STEP 12: Aggregate Results Across All Bootstrap Iterations\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "        \n",
    "    \n",
    "print(\"BOOTSTRAP SUMMARY: Aggregated Results Across 5 Iterations\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "F1_scores = [r['F1'] for r in bootstrap_results]\n",
    "PQ_scores = [r['PQ'] for r in bootstrap_results]\n",
    "PC_scores = [r['PC'] for r in bootstrap_results]\n",
    "\n",
    "print(f\"\\nF1 Score Statistics:\")\n",
    "print(f\"  Mean: {np.mean(F1_scores):.6f} ({np.mean(F1_scores)*100:.4f}%)\")\n",
    "print(f\"  Std:  {np.std(F1_scores):.6f} ({np.std(F1_scores)*100:.4f}%)\")\n",
    "print(f\"  Min:  {np.min(F1_scores):.6f} ({np.min(F1_scores)*100:.4f}%)\")\n",
    "print(f\"  Max:  {np.max(F1_scores):.6f} ({np.max(F1_scores)*100:.4f}%)\")\n",
    "\n",
    "print(f\"\\nPair Quality (PQ) Statistics:\")\n",
    "print(f\"  Mean: {np.mean(PQ_scores):.6f} ({np.mean(PQ_scores)*100:.4f}%)\")\n",
    "print(f\"  Std:  {np.std(PQ_scores):.6f} ({np.std(PQ_scores)*100:.4f}%)\")\n",
    "\n",
    "print(f\"\\nPair Completeness (PC) Statistics:\")\n",
    "print(f\"  Mean: {np.mean(PC_scores):.6f} ({np.mean(PC_scores)*100:.4f}%)\")\n",
    "print(f\"  Std:  {np.std(PC_scores):.6f} ({np.std(PC_scores)*100:.4f}%)\")\n",
    "\n",
    "print(f\"\\nDetailed Results per Iteration:\")\n",
    "print(f\"{'Iter':<6} {'Threshold':<10} {'F1':<12} {'PQ':<12} {'PC':<12} {'TP':<6} {'Candidates':<12}\")\n",
    "print(\"-\" * 80)\n",
    "for r in bootstrap_results:\n",
    "    print(f\"{r['iteration']:<6} {r.get('threshold', 'N/A'):<10.2f} {r['F1']:<12.6f} {r['PQ']:<12.6f} {r['PC']:<12.6f} \"\n",
    "          f\"{r['TP']:<6} {r['candidate_pairs']:<12}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Store final aggregated results\n",
    "bootstrap_summary = {\n",
    "    'n_iterations': 5,\n",
    "    'F1_mean': np.mean(F1_scores),\n",
    "    'F1_std': np.std(F1_scores),\n",
    "    'F1_min': np.min(F1_scores),\n",
    "    'F1_max': np.max(F1_scores),\n",
    "    'PQ_mean': np.mean(PQ_scores),\n",
    "    'PQ_std': np.std(PQ_scores),\n",
    "    'PC_mean': np.mean(PC_scores),\n",
    "    'PC_std': np.std(PC_scores),\n",
    "    'all_results': bootstrap_results\n",
    "}\n",
    "\n",
    "print(f\"\\nBootstrap evaluation complete!\")\n",
    "print(f\"Final F1 Score: {bootstrap_summary['F1_mean']:.6f} ± {bootstrap_summary['F1_std']:.6f}\")\n",
    "print(\"=\"*70)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "clustering_without_lsh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CLUSTERING WITHOUT LSH: Bootstrap Evaluation (5 Iterations)\n",
      "======================================================================\n",
      "Total samples: 1624\n",
      "Number of bootstrap iterations: 5\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "BOOTSTRAP ITERATION 1/5\n",
      "======================================================================\n",
      "\n",
      "--- Bootstrap 1 Split ---\n",
      "  Training set: 1624 samples (1016 unique)\n",
      "  Test set: 608 samples (608 unique)\n",
      "\n",
      "--- Finding Best Clustering Threshold on Training Set ---\n",
      "  Building full similarity matrix for training set...\n",
      "  Computing similarity for 515620 pairs...\n",
      "    Computed similarities for 100/1016 products...\n",
      "    Computed similarities for 200/1016 products...\n",
      "    Computed similarities for 300/1016 products...\n",
      "    Computed similarities for 400/1016 products...\n",
      "    Computed similarities for 500/1016 products...\n",
      "    Computed similarities for 600/1016 products...\n",
      "    Computed similarities for 700/1016 products...\n",
      "    Computed similarities for 800/1016 products...\n",
      "    Computed similarities for 900/1016 products...\n",
      "    Computed similarities for 1000/1016 products...\n",
      "  Completed computing TMWM similarity for 515620 pairs\n",
      "\n",
      "  Testing 7 distance thresholds on training set...\n",
      "  Threshold    Clusters   TP     PQ         PC         F1        \n",
      "  --------------------------------------------------------------------\n",
      "  0.20         891        3      0.012295   0.019355   0.015038  \n",
      "  0.30         737        4      0.005502   0.025806   0.009070  \n",
      "  0.40         656        5      0.005663   0.032258   0.009634  \n",
      "  0.50         561        5      0.004488   0.032258   0.007880  \n",
      "  0.60         349        4      0.001358   0.025806   0.002581  \n",
      "  0.70         251        3      0.000656   0.019355   0.001269  \n",
      "  0.80         186        3      0.000321   0.019355   0.000631  \n",
      "\n",
      "  Best threshold: 0.2 (F1 on training: 0.015038)\n",
      "\n",
      "--- Applying Clustering to Test Set ---\n",
      "  Building full similarity matrix for test set...\n",
      "  Computing similarity for 184528 pairs...\n",
      "    Computed similarities for 100/608 products...\n",
      "    Computed similarities for 200/608 products...\n",
      "    Computed similarities for 300/608 products...\n",
      "    Computed similarities for 400/608 products...\n",
      "    Computed similarities for 500/608 products...\n",
      "    Computed similarities for 600/608 products...\n",
      "  Completed computing TMWM similarity for 184528 pairs\n",
      "  Applying clustering with distance threshold: 0.2\n",
      "\n",
      "  Clustering Results (Test Set):\n",
      "    Number of clusters: 547\n",
      "    Clusters with 2+ products: 44\n",
      "    True positives (TP): 1\n",
      "    Total predicted pairs: 95\n",
      "    Total true pairs: 62\n",
      "    Precision (PQ): 0.010526 (1.0526%)\n",
      "    Recall (PC): 0.016129 (1.6129%)\n",
      "    F1 Score: 0.012739 (1.2739%)\n",
      "\n",
      "--- Bootstrap Iteration 1 Complete ---\n",
      "\n",
      "======================================================================\n",
      "BOOTSTRAP ITERATION 2/5\n",
      "======================================================================\n",
      "\n",
      "--- Bootstrap 2 Split ---\n",
      "  Training set: 1624 samples (1009 unique)\n",
      "  Test set: 615 samples (615 unique)\n",
      "\n",
      "--- Finding Best Clustering Threshold on Training Set ---\n",
      "  Building full similarity matrix for training set...\n",
      "  Computing similarity for 508536 pairs...\n",
      "    Computed similarities for 100/1009 products...\n",
      "    Computed similarities for 200/1009 products...\n",
      "    Computed similarities for 300/1009 products...\n",
      "    Computed similarities for 400/1009 products...\n",
      "    Computed similarities for 500/1009 products...\n",
      "    Computed similarities for 600/1009 products...\n",
      "    Computed similarities for 700/1009 products...\n",
      "    Computed similarities for 800/1009 products...\n",
      "    Computed similarities for 900/1009 products...\n",
      "    Computed similarities for 1000/1009 products...\n",
      "  Completed computing TMWM similarity for 508536 pairs\n",
      "\n",
      "  Testing 7 distance thresholds on training set...\n",
      "  Threshold    Clusters   TP     PQ         PC         F1        \n",
      "  --------------------------------------------------------------------\n",
      "  0.20         879        1      0.004132   0.006849   0.005155  \n",
      "  0.30         743        2      0.003241   0.013699   0.005242  \n",
      "  0.40         661        3      0.003793   0.020548   0.006403  \n",
      "  0.50         554        5      0.004529   0.034247   0.008000  \n",
      "  0.60         358        6      0.002155   0.041096   0.004096  \n",
      "  0.70         258        5      0.001093   0.034247   0.002118  \n",
      "  0.80         181        3      0.000272   0.020548   0.000538  \n",
      "\n",
      "  Best threshold: 0.5 (F1 on training: 0.008000)\n",
      "\n",
      "--- Applying Clustering to Test Set ---\n",
      "  Building full similarity matrix for test set...\n",
      "  Computing similarity for 188805 pairs...\n",
      "    Computed similarities for 100/615 products...\n",
      "    Computed similarities for 200/615 products...\n",
      "    Computed similarities for 300/615 products...\n",
      "    Computed similarities for 400/615 products...\n",
      "    Computed similarities for 500/615 products...\n",
      "    Computed similarities for 600/615 products...\n",
      "  Completed computing TMWM similarity for 188805 pairs\n",
      "  Applying clustering with distance threshold: 0.5\n",
      "\n",
      "  Clustering Results (Test Set):\n",
      "    Number of clusters: 361\n",
      "    Clusters with 2+ products: 142\n",
      "    True positives (TP): 2\n",
      "    Total predicted pairs: 541\n",
      "    Total true pairs: 56\n",
      "    Precision (PQ): 0.003697 (0.3697%)\n",
      "    Recall (PC): 0.035714 (3.5714%)\n",
      "    F1 Score: 0.006700 (0.6700%)\n",
      "\n",
      "--- Bootstrap Iteration 2 Complete ---\n",
      "\n",
      "======================================================================\n",
      "BOOTSTRAP ITERATION 3/5\n",
      "======================================================================\n",
      "\n",
      "--- Bootstrap 3 Split ---\n",
      "  Training set: 1624 samples (1017 unique)\n",
      "  Test set: 607 samples (607 unique)\n",
      "\n",
      "--- Finding Best Clustering Threshold on Training Set ---\n",
      "  Building full similarity matrix for training set...\n",
      "  Computing similarity for 516636 pairs...\n",
      "    Computed similarities for 100/1017 products...\n",
      "    Computed similarities for 200/1017 products...\n",
      "    Computed similarities for 300/1017 products...\n",
      "    Computed similarities for 400/1017 products...\n",
      "    Computed similarities for 500/1017 products...\n",
      "    Computed similarities for 600/1017 products...\n",
      "    Computed similarities for 700/1017 products...\n",
      "    Computed similarities for 800/1017 products...\n",
      "    Computed similarities for 900/1017 products...\n",
      "    Computed similarities for 1000/1017 products...\n",
      "  Completed computing TMWM similarity for 516636 pairs\n",
      "\n",
      "  Testing 7 distance thresholds on training set...\n",
      "  Threshold    Clusters   TP     PQ         PC         F1        \n",
      "  --------------------------------------------------------------------\n",
      "  0.20         882        3      0.011070   0.019108   0.014019  \n",
      "  0.30         735        5      0.007407   0.031847   0.012019  \n",
      "  0.40         646        7      0.008092   0.044586   0.013699  \n",
      "  0.50         552        8      0.006717   0.050955   0.011869  \n",
      "  0.60         339        6      0.001980   0.038217   0.003764  \n",
      "  0.70         250        5      0.001095   0.031847   0.002118  \n",
      "  0.80         184        4      0.000421   0.025478   0.000829  \n",
      "\n",
      "  Best threshold: 0.2 (F1 on training: 0.014019)\n",
      "\n",
      "--- Applying Clustering to Test Set ---\n",
      "  Building full similarity matrix for test set...\n",
      "  Computing similarity for 183921 pairs...\n",
      "    Computed similarities for 100/607 products...\n",
      "    Computed similarities for 200/607 products...\n",
      "    Computed similarities for 300/607 products...\n",
      "    Computed similarities for 400/607 products...\n",
      "    Computed similarities for 500/607 products...\n",
      "    Computed similarities for 600/607 products...\n",
      "  Completed computing TMWM similarity for 183921 pairs\n",
      "  Applying clustering with distance threshold: 0.2\n",
      "\n",
      "  Clustering Results (Test Set):\n",
      "    Number of clusters: 557\n",
      "    Clusters with 2+ products: 31\n",
      "    True positives (TP): 1\n",
      "    Total predicted pairs: 101\n",
      "    Total true pairs: 56\n",
      "    Precision (PQ): 0.009901 (0.9901%)\n",
      "    Recall (PC): 0.017857 (1.7857%)\n",
      "    F1 Score: 0.012739 (1.2739%)\n",
      "\n",
      "--- Bootstrap Iteration 3 Complete ---\n",
      "\n",
      "======================================================================\n",
      "BOOTSTRAP ITERATION 4/5\n",
      "======================================================================\n",
      "\n",
      "--- Bootstrap 4 Split ---\n",
      "  Training set: 1624 samples (1051 unique)\n",
      "  Test set: 573 samples (573 unique)\n",
      "\n",
      "--- Finding Best Clustering Threshold on Training Set ---\n",
      "  Building full similarity matrix for training set...\n",
      "  Computing similarity for 551775 pairs...\n",
      "    Computed similarities for 100/1051 products...\n",
      "    Computed similarities for 200/1051 products...\n",
      "    Computed similarities for 300/1051 products...\n",
      "    Computed similarities for 400/1051 products...\n",
      "    Computed similarities for 500/1051 products...\n",
      "    Computed similarities for 600/1051 products...\n",
      "    Computed similarities for 700/1051 products...\n",
      "    Computed similarities for 800/1051 products...\n",
      "    Computed similarities for 900/1051 products...\n",
      "    Computed similarities for 1000/1051 products...\n",
      "  Completed computing TMWM similarity for 551775 pairs\n",
      "\n",
      "  Testing 7 distance thresholds on training set...\n",
      "  Threshold    Clusters   TP     PQ         PC         F1        \n",
      "  --------------------------------------------------------------------\n",
      "  0.20         900        4      0.012121   0.023121   0.015905  \n",
      "  0.30         746        6      0.007042   0.034682   0.011707  \n",
      "  0.40         661        6      0.005854   0.034682   0.010017  \n",
      "  0.50         554        8      0.005922   0.046243   0.010499  \n",
      "  0.60         343        3      0.000866   0.017341   0.001649  \n",
      "  0.70         246        4      0.000760   0.023121   0.001472  \n",
      "  0.80         180        3      0.000280   0.017341   0.000551  \n",
      "\n",
      "  Best threshold: 0.2 (F1 on training: 0.015905)\n",
      "\n",
      "--- Applying Clustering to Test Set ---\n",
      "  Building full similarity matrix for test set...\n",
      "  Computing similarity for 163878 pairs...\n",
      "    Computed similarities for 100/573 products...\n",
      "    Computed similarities for 200/573 products...\n",
      "    Computed similarities for 300/573 products...\n",
      "    Computed similarities for 400/573 products...\n",
      "    Computed similarities for 500/573 products...\n",
      "  Completed computing TMWM similarity for 163878 pairs\n",
      "  Applying clustering with distance threshold: 0.2\n",
      "\n",
      "  Clustering Results (Test Set):\n",
      "    Number of clusters: 529\n",
      "    Clusters with 2+ products: 26\n",
      "    True positives (TP): 1\n",
      "    Total predicted pairs: 88\n",
      "    Total true pairs: 54\n",
      "    Precision (PQ): 0.011364 (1.1364%)\n",
      "    Recall (PC): 0.018519 (1.8519%)\n",
      "    F1 Score: 0.014085 (1.4085%)\n",
      "\n",
      "--- Bootstrap Iteration 4 Complete ---\n",
      "\n",
      "======================================================================\n",
      "BOOTSTRAP ITERATION 5/5\n",
      "======================================================================\n",
      "\n",
      "--- Bootstrap 5 Split ---\n",
      "  Training set: 1624 samples (1022 unique)\n",
      "  Test set: 602 samples (602 unique)\n",
      "\n",
      "--- Finding Best Clustering Threshold on Training Set ---\n",
      "  Building full similarity matrix for training set...\n",
      "  Computing similarity for 521731 pairs...\n",
      "    Computed similarities for 100/1022 products...\n",
      "    Computed similarities for 200/1022 products...\n",
      "    Computed similarities for 300/1022 products...\n",
      "    Computed similarities for 400/1022 products...\n",
      "    Computed similarities for 500/1022 products...\n",
      "    Computed similarities for 600/1022 products...\n",
      "    Computed similarities for 700/1022 products...\n",
      "    Computed similarities for 800/1022 products...\n",
      "    Computed similarities for 900/1022 products...\n",
      "    Computed similarities for 1000/1022 products...\n",
      "  Completed computing TMWM similarity for 521731 pairs\n",
      "\n",
      "  Testing 7 distance thresholds on training set...\n",
      "  Threshold    Clusters   TP     PQ         PC         F1        \n",
      "  --------------------------------------------------------------------\n",
      "  0.20         902        2      0.008584   0.013072   0.010363  \n",
      "  0.30         750        3      0.004208   0.019608   0.006928  \n",
      "  0.40         658        5      0.005543   0.032680   0.009479  \n",
      "  0.50         561        6      0.005068   0.039216   0.008975  \n",
      "  0.60         361        5      0.001775   0.032680   0.003367  \n",
      "  0.70         257        4      0.000837   0.026144   0.001622  \n",
      "  0.80         190        3      0.000312   0.019608   0.000615  \n",
      "\n",
      "  Best threshold: 0.2 (F1 on training: 0.010363)\n",
      "\n",
      "--- Applying Clustering to Test Set ---\n",
      "  Building full similarity matrix for test set...\n",
      "  Computing similarity for 180901 pairs...\n",
      "    Computed similarities for 100/602 products...\n",
      "    Computed similarities for 200/602 products...\n",
      "    Computed similarities for 300/602 products...\n",
      "    Computed similarities for 400/602 products...\n",
      "    Computed similarities for 500/602 products...\n",
      "    Computed similarities for 600/602 products...\n",
      "  Completed computing TMWM similarity for 180901 pairs\n",
      "  Applying clustering with distance threshold: 0.2\n",
      "\n",
      "  Clustering Results (Test Set):\n",
      "    Number of clusters: 533\n",
      "    Clusters with 2+ products: 47\n",
      "    True positives (TP): 1\n",
      "    Total predicted pairs: 131\n",
      "    Total true pairs: 61\n",
      "    Precision (PQ): 0.007634 (0.7634%)\n",
      "    Recall (PC): 0.016393 (1.6393%)\n",
      "    F1 Score: 0.010417 (1.0417%)\n",
      "\n",
      "--- Bootstrap Iteration 5 Complete ---\n",
      "\n",
      "======================================================================\n",
      "CLUSTERING WITHOUT LSH: SUMMARY STATISTICS\n",
      "======================================================================\n",
      "\n",
      "F1 Score Statistics:\n",
      "  Mean: 0.011336 ± 0.002601 (1.1336% ± 0.2601%)\n",
      "  Min:  0.006700 (0.6700%)\n",
      "  Max:  0.014085 (1.4085%)\n",
      "\n",
      "Precision (PQ) Statistics:\n",
      "  Mean: 0.008624 ± 0.002757\n",
      "\n",
      "Recall (PC) Statistics:\n",
      "  Mean: 0.020922 ± 0.007449\n",
      "\n",
      "Detailed Results per Iteration:\n",
      "Iter   Thresh   F1 Train   F1 Test    PQ         PC         TP     Clusters  \n",
      "--------------------------------------------------------------------------------\n",
      "1      0.20     0.015038   0.012739   0.010526   0.016129   1      547       \n",
      "2      0.50     0.008000   0.006700   0.003697   0.035714   2      361       \n",
      "3      0.20     0.014019   0.012739   0.009901   0.017857   1      557       \n",
      "4      0.20     0.015905   0.014085   0.011364   0.018519   1      529       \n",
      "5      0.20     0.010363   0.010417   0.007634   0.016393   1      533       \n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CLUSTERING WITHOUT LSH: Bootstrap Evaluation (5 Iterations)\n",
    "# ============================================================================\n",
    "# This evaluates clustering directly on all products without using LSH\n",
    "# to generate candidate pairs first.\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "def build_full_similarity_matrix(all_product_ids, dict_data, verbose=True):\n",
    "    \"\"\"\n",
    "    Build a full similarity matrix for ALL products (not just filtered pairs).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    all_product_ids : list\n",
    "        List of all product IDs\n",
    "    dict_data : dict\n",
    "        Dictionary mapping product IDs to product data\n",
    "    verbose : bool\n",
    "        Whether to print progress\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    similarity_matrix : np.ndarray\n",
    "        Full TMWM similarity matrix for all products\n",
    "    product_id_to_index : dict\n",
    "        Mapping from product ID to matrix index\n",
    "    \"\"\"\n",
    "    n = len(all_product_ids)\n",
    "    product_id_to_index = {pid: idx for idx, pid in enumerate(all_product_ids)}\n",
    "    \n",
    "    # Initialize similarity matrix (all zeros)\n",
    "    similarity_matrix = np.zeros((n, n), dtype=float)\n",
    "    \n",
    "    # Set diagonal to 1.0 (self-similarity)\n",
    "    np.fill_diagonal(similarity_matrix, 1.0)\n",
    "    \n",
    "    # Compute TMWM similarity for ALL pairs\n",
    "    total_pairs = n * (n - 1) // 2\n",
    "    computed = 0\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Computing similarity for {total_pairs} pairs...\")\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            product_i = all_product_ids[i]\n",
    "            product_j = all_product_ids[j]\n",
    "            \n",
    "            item_i = dict_data.get(product_i)\n",
    "            item_j = dict_data.get(product_j)\n",
    "            \n",
    "            if item_i and item_j:\n",
    "                # Compute TMWM similarity\n",
    "                tmwm_sim = compute_tmwm_similarity(item_i, item_j)\n",
    "                similarity_matrix[i, j] = tmwm_sim\n",
    "                similarity_matrix[j, i] = tmwm_sim  # Symmetric\n",
    "                computed += 1\n",
    "        \n",
    "        if verbose and (i + 1) % 100 == 0:\n",
    "            print(f\"    Computed similarities for {i + 1}/{n} products...\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Completed computing TMWM similarity for {computed} pairs\")\n",
    "    \n",
    "    return similarity_matrix, product_id_to_index\n",
    "\n",
    "\n",
    "# Clean the data first\n",
    "all_values_dict_clean = clean_all_values_dict_advanced(all_values_dict, remove_shops=True)\n",
    "n_samples = len(all_values_dict_clean)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CLUSTERING WITHOUT LSH: Bootstrap Evaluation (5 Iterations)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total samples: {n_samples}\")\n",
    "print(f\"Number of bootstrap iterations: 5\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Store results for each bootstrap iteration\n",
    "clustering_no_lsh_results = []\n",
    "\n",
    "# Run 5 bootstrap iterations\n",
    "for bootstrap_idx in range(5):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"BOOTSTRAP ITERATION {bootstrap_idx + 1}/5\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 1: Create Bootstrap Train/Test Split\n",
    "    # ========================================================================\n",
    "    train_indices, test_indices = bootstrap_train_test_split(n_samples)\n",
    "    \n",
    "    # Get unique indices (remove duplicates from bootstrap sampling)\n",
    "    unique_train_indices = np.unique(train_indices)\n",
    "    unique_test_indices = np.unique(test_indices)\n",
    "    \n",
    "    # Create train and test dictionaries using only unique products\n",
    "    dict_train = {i: all_values_dict_clean[idx] for i, idx in enumerate(unique_train_indices)}\n",
    "    dict_test = {i: all_values_dict_clean[idx] for i, idx in enumerate(unique_test_indices)}\n",
    "    \n",
    "    print(f\"\\n--- Bootstrap {bootstrap_idx + 1} Split ---\")\n",
    "    print(f\"  Training set: {len(train_indices)} samples ({len(unique_train_indices)} unique)\")\n",
    "    print(f\"  Test set: {len(test_indices)} samples ({len(unique_test_indices)} unique)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 2: Find Best Clustering Threshold on Training Set\n",
    "    # ========================================================================\n",
    "    print(f\"\\n--- Finding Best Clustering Threshold on Training Set ---\")\n",
    "    \n",
    "    all_product_ids_train = list(dict_train.keys())\n",
    "    \n",
    "    # Build full similarity matrix for training set\n",
    "    print(\"  Building full similarity matrix for training set...\")\n",
    "    similarity_matrix_train, _ = build_full_similarity_matrix(\n",
    "        all_product_ids_train, dict_train, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Convert similarity to distance\n",
    "    distance_matrix_train = 1.0 - similarity_matrix_train\n",
    "    \n",
    "    # Test different distance thresholds on training set\n",
    "    distance_thresholds = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "    best_threshold = None\n",
    "    best_f1_train = -1\n",
    "    threshold_results_train = {}\n",
    "    \n",
    "    print(f\"\\n  Testing {len(distance_thresholds)} distance thresholds on training set...\")\n",
    "    print(f\"  {'Threshold':<12} {'Clusters':<10} {'TP':<6} {'PQ':<10} {'PC':<10} {'F1':<10}\")\n",
    "    print(\"  \" + \"-\" * 68)\n",
    "    \n",
    "    for dist_thresh in distance_thresholds:\n",
    "        # Apply clustering with this threshold\n",
    "        clustering = AgglomerativeClustering(\n",
    "            n_clusters=None,\n",
    "            metric='precomputed',\n",
    "            linkage='complete',\n",
    "            distance_threshold=dist_thresh,\n",
    "            compute_full_tree=True\n",
    "        )\n",
    "        \n",
    "        cluster_labels = clustering.fit_predict(distance_matrix_train)\n",
    "        \n",
    "        # Evaluate clustering\n",
    "        f1, precision, recall, TP, total_predicted_pairs, total_true_pairs = evaluate_clustering_f1_score(\n",
    "            cluster_labels, dict_train, all_product_ids_train\n",
    "        )\n",
    "        \n",
    "        threshold_results_train[dist_thresh] = {\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'TP': TP,\n",
    "            'n_clusters': len(set(cluster_labels)),\n",
    "            'predicted_pairs': total_predicted_pairs,\n",
    "            'true_pairs': total_true_pairs\n",
    "        }\n",
    "        \n",
    "        print(f\"  {dist_thresh:<12.2f} {len(set(cluster_labels)):<10} {TP:<6} {precision:<10.6f} {recall:<10.6f} {f1:<10.6f}\")\n",
    "        \n",
    "        # Update best threshold (prefer F1 <= 0.2 to avoid overfitting)\n",
    "        if f1 <= 0.2 and f1 > best_f1_train:\n",
    "            best_f1_train = f1\n",
    "            best_threshold = dist_thresh\n",
    "    \n",
    "    if best_threshold is None:\n",
    "        # If no threshold found with F1 <= 0.2, use the best available\n",
    "        best_threshold = max(threshold_results_train.keys(), key=lambda t: threshold_results_train[t]['f1'])\n",
    "        best_f1_train = threshold_results_train[best_threshold]['f1']\n",
    "    \n",
    "    print(f\"\\n  Best threshold: {best_threshold} (F1 on training: {best_f1_train:.6f})\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 3: Apply Clustering to Test Set with Best Threshold\n",
    "    # ========================================================================\n",
    "    print(f\"\\n--- Applying Clustering to Test Set ---\")\n",
    "    \n",
    "    all_product_ids_test = list(dict_test.keys())\n",
    "    \n",
    "    # Build full similarity matrix for test set\n",
    "    print(\"  Building full similarity matrix for test set...\")\n",
    "    similarity_matrix_test, _ = build_full_similarity_matrix(\n",
    "        all_product_ids_test, dict_test, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Convert similarity to distance\n",
    "    distance_matrix_test = 1.0 - similarity_matrix_test\n",
    "    \n",
    "    # Apply clustering with best threshold\n",
    "    print(f\"  Applying clustering with distance threshold: {best_threshold}\")\n",
    "    clustering = AgglomerativeClustering(\n",
    "        n_clusters=None,\n",
    "        metric='precomputed',\n",
    "        linkage='complete',\n",
    "        distance_threshold=best_threshold,\n",
    "        compute_full_tree=True\n",
    "    )\n",
    "    \n",
    "    cluster_labels = clustering.fit_predict(distance_matrix_test)\n",
    "    \n",
    "    # Evaluate clustering on test set\n",
    "    f1_test, precision_test, recall_test, TP_test, total_predicted_pairs_test, total_true_pairs_test = evaluate_clustering_f1_score(\n",
    "        cluster_labels, dict_test, all_product_ids_test\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n  Clustering Results (Test Set):\")\n",
    "    print(f\"    Number of clusters: {len(set(cluster_labels))}\")\n",
    "    print(f\"    Clusters with 2+ products: {sum(1 for c in set(cluster_labels) if list(cluster_labels).count(c) > 1)}\")\n",
    "    print(f\"    True positives (TP): {TP_test}\")\n",
    "    print(f\"    Total predicted pairs: {total_predicted_pairs_test}\")\n",
    "    print(f\"    Total true pairs: {total_true_pairs_test}\")\n",
    "    print(f\"    Precision (PQ): {precision_test:.6f} ({precision_test*100:.4f}%)\")\n",
    "    print(f\"    Recall (PC): {recall_test:.6f} ({recall_test*100:.4f}%)\")\n",
    "    print(f\"    F1 Score: {f1_test:.6f} ({f1_test*100:.4f}%)\")\n",
    "    \n",
    "    # Store results\n",
    "    clustering_no_lsh_results.append({\n",
    "        'iteration': bootstrap_idx + 1,\n",
    "        'best_threshold': best_threshold,\n",
    "        'best_f1_train': best_f1_train,\n",
    "        'f1_test': f1_test,\n",
    "        'precision_test': precision_test,\n",
    "        'recall_test': recall_test,\n",
    "        'TP_test': TP_test,\n",
    "        'total_predicted_pairs_test': total_predicted_pairs_test,\n",
    "        'total_true_pairs_test': total_true_pairs_test,\n",
    "        'n_clusters': len(set(cluster_labels)),\n",
    "        'train_size': len(unique_train_indices),\n",
    "        'test_size': len(unique_test_indices)\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n--- Bootstrap Iteration {bootstrap_idx + 1} Complete ---\")\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 4: Summary Statistics\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLUSTERING WITHOUT LSH: SUMMARY STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if clustering_no_lsh_results:\n",
    "    f1_scores = [r['f1_test'] for r in clustering_no_lsh_results]\n",
    "    precision_scores = [r['precision_test'] for r in clustering_no_lsh_results]\n",
    "    recall_scores = [r['recall_test'] for r in clustering_no_lsh_results]\n",
    "    \n",
    "    print(f\"\\nF1 Score Statistics:\")\n",
    "    print(f\"  Mean: {np.mean(f1_scores):.6f} ± {np.std(f1_scores):.6f} ({np.mean(f1_scores)*100:.4f}% ± {np.std(f1_scores)*100:.4f}%)\")\n",
    "    print(f\"  Min:  {np.min(f1_scores):.6f} ({np.min(f1_scores)*100:.4f}%)\")\n",
    "    print(f\"  Max:  {np.max(f1_scores):.6f} ({np.max(f1_scores)*100:.4f}%)\")\n",
    "    \n",
    "    print(f\"\\nPrecision (PQ) Statistics:\")\n",
    "    print(f\"  Mean: {np.mean(precision_scores):.6f} ± {np.std(precision_scores):.6f}\")\n",
    "    \n",
    "    print(f\"\\nRecall (PC) Statistics:\")\n",
    "    print(f\"  Mean: {np.mean(recall_scores):.6f} ± {np.std(recall_scores):.6f}\")\n",
    "    \n",
    "    print(f\"\\nDetailed Results per Iteration:\")\n",
    "    print(f\"{'Iter':<6} {'Thresh':<8} {'F1 Train':<10} {'F1 Test':<10} {'PQ':<10} {'PC':<10} {'TP':<6} {'Clusters':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    for r in clustering_no_lsh_results:\n",
    "        print(f\"{r['iteration']:<6} {r['best_threshold']:<8.2f} {r['best_f1_train']:<10.6f} {r['f1_test']:<10.6f} \"\n",
    "              f\"{r['precision_test']:<10.6f} {r['recall_test']:<10.6f} {r['TP_test']:<6} {r['n_clusters']:<10}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"No results found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
